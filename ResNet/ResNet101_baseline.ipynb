{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "170f69e4-bf99-4e8e-926a-710a8a86c3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 26 08:55:01 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   41C    P0    37W / 250W |      0MiB / 32510MiB |      9%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1d65a3-78de-4b01-a862-e6bd4c7ef135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d15beaba-39cc-48da-8165-9f2abc16322a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/ml/input/data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('input/data')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b3beb99-f9c7-4026-989a-cbaa8de1f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find . -regex \".*\\.\\_[a-zA-Z0-9._]+\" -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3dab5a-b5ec-498e-99db-604a1fe578c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/ml'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa8d19de-5a8e-454f-a22b-129e94792395",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 12\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f69f93c-5675-433f-8a88-7bf8e411d51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>006954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>006955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>006956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>006957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path\n",
       "0     000001  female  Asian   45  000001_female_Asian_45\n",
       "1     000002  female  Asian   52  000002_female_Asian_52\n",
       "2     000004    male  Asian   54    000004_male_Asian_54\n",
       "3     000005  female  Asian   58  000005_female_Asian_58\n",
       "4     000006  female  Asian   59  000006_female_Asian_59\n",
       "...      ...     ...    ...  ...                     ...\n",
       "2695  006954    male  Asian   19    006954_male_Asian_19\n",
       "2696  006955    male  Asian   19    006955_male_Asian_19\n",
       "2697  006956    male  Asian   19    006956_male_Asian_19\n",
       "2698  006957    male  Asian   20    006957_male_Asian_20\n",
       "2699  006959    male  Asian   19    006959_male_Asian_19\n",
       "\n",
       "[2700 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir_path = '/opt/ml/input/data/train/'\n",
    "train_image_path = '/opt/ml/input/data/train/images/'\n",
    "\n",
    "dt_train = pd.read_csv(train_dir_path+'train.csv')\n",
    "dt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "304c6a61-ff17-4777-8132-18def61c8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_image_path = []\n",
    "whole_target_label = []\n",
    "\n",
    "for path in dt_train['path']:\n",
    "    for file_name in [i for i in os.listdir(train_image_path+path) if './' not in i]:\n",
    "        whole_image_path.append(train_image_path+path+'/'+file_name)\n",
    "        whole_target_label.append((path.split('_')[1], path.split('_')[3], file_name.split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98de1eb4-55d4-41ba-8541-70fff9bb011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_enc(x):\n",
    "    def gender(i):\n",
    "        if i == 'male':\n",
    "            return 0\n",
    "        elif i == 'female':\n",
    "            return 3\n",
    "    def age(j):\n",
    "        j = int(j)\n",
    "        if j < 30:\n",
    "            return 0\n",
    "        elif j >= 30 and j < 60:\n",
    "            return 1\n",
    "        elif j >= 60:\n",
    "            return 2\n",
    "    def mask(k):\n",
    "        if k == 'normal':\n",
    "            return 12\n",
    "        elif 'incorrect' in k:\n",
    "            return 6\n",
    "        else:\n",
    "            return 0\n",
    "    return gender(x[0]) + age(x[1]) + mask(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "694474c7-7175-40e6-b071-e50218629f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_data = pd.Series(whole_image_path)\n",
    "sr_label = pd.Series(whole_target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2cd7b29-5f03-444e-b781-902ca74f2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Mask(Dataset):\n",
    "    def __init__(self, encoding=True, midcrop=True, transform=None):\n",
    "        self.encoding = encoding\n",
    "        self.midcrop = midcrop\n",
    "        self.data = sr_data\n",
    "        self.label = sr_label\n",
    "        self.transform = transform\n",
    "        \n",
    "        if encoding:\n",
    "            self.label = self.label.apply(onehot_enc)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(sr_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        y = self.label[idx]\n",
    "        \n",
    "        if self.midcrop:\n",
    "            X = X[64:448]\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(X), y\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2a22aff-e634-4f15-877c-752d450d01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mask = Dataset_Mask(transform = transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5788c44b-07ac-4820-a22d-b6ffa3fc81f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset_mask) * 0.8)\n",
    "val_size = int(len(dataset_mask) * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9dedcee-2cba-4c68-9e49-e12112a09851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18900\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e285cb6-ee37-4a0c-a804-7506dcfebc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size : 15120\n",
      "validation data size : 3780\n"
     ]
    }
   ],
   "source": [
    "mask_train_set, mask_val_set = torch.utils.data.random_split(dataset_mask, [train_size, val_size])\n",
    "print(f'training data size : {len(mask_train_set)}')\n",
    "print(f'validation data size : {len(mask_val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37b1ced0-1e34-4d50-b2b2-dcdff8fd33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader_mask = DataLoader(dataset = mask_train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_dataloader_mask = DataLoader(dataset = mask_val_set, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af71a4ef-a137-47b0-af94-eebedaa36014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필요 입력 채널 개수 3\n",
      "네트워크 출력 채널 개수 1000\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (13): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (14): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (15): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (16): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (17): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (18): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (19): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (20): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (21): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (22): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "basemodel_resnet101 = torchvision.models.resnet101(pretrained=True)\n",
    "print('필요 입력 채널 개수', basemodel_resnet101.conv1.weight.shape[1])\n",
    "print('네트워크 출력 채널 개수', basemodel_resnet101.fc.weight.shape[0])\n",
    "print(basemodel_resnet101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11357fc4-548b-452e-bbd8-e60ef6261308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필요 입력 채널 개수 3\n",
      "네트워크 출력 채널 개수 18\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "class_num = 18\n",
    "basemodel_resnet101.fc = nn.Linear(in_features=2048, out_features=class_num, bias=True)\n",
    "nn.init.xavier_uniform_(basemodel_resnet101.fc.weight)\n",
    "stdv = 1. / math.sqrt(basemodel_resnet101.fc.weight.size(1))\n",
    "basemodel_resnet101.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "print('필요 입력 채널 개수', basemodel_resnet101.conv1.weight.shape[1])\n",
    "print('네트워크 출력 채널 개수', basemodel_resnet101.fc.weight.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a1a2262-f820-4294-8086-3a48a5e5d686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")\n",
    "\n",
    "basemodel_resnet101.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCH = 30\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(basemodel_resnet101.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64a36f92-aad5-4bbb-a920-29d50f0f058b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] name:[conv1.weight] shape:[(64, 3, 7, 7)].\n",
      "    val:[ 0.02  -0.004 -0.018 -0.028 -0.015]\n",
      "[1] name:[bn1.weight] shape:[(64,)].\n",
      "    val:[0.261 0.195 0.273 0.425 0.283]\n",
      "[2] name:[bn1.bias] shape:[(64,)].\n",
      "    val:[0.197 0.223 0.184 1.066 0.681]\n",
      "[3] name:[layer1.0.conv1.weight] shape:[(64, 64, 1, 1)].\n",
      "    val:[-0.187  0.024 -0.022 -0.055  0.022]\n",
      "[4] name:[layer1.0.bn1.weight] shape:[(64,)].\n",
      "    val:[2.146e-01 1.453e-01 6.095e-09 1.160e-01 2.530e-01]\n",
      "[5] name:[layer1.0.bn1.bias] shape:[(64,)].\n",
      "    val:[ 4.568e-02 -3.505e-02 -3.354e-08  1.901e-01 -3.316e-03]\n",
      "[6] name:[layer1.0.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 0.013 -0.045  0.027  0.028  0.067]\n",
      "[7] name:[layer1.0.bn2.weight] shape:[(64,)].\n",
      "    val:[0.139 0.149 0.148 0.132 0.155]\n",
      "[8] name:[layer1.0.bn2.bias] shape:[(64,)].\n",
      "    val:[ 7.826e-02 -2.417e-04  3.763e-01  6.085e-02 -3.057e-02]\n",
      "[9] name:[layer1.0.conv3.weight] shape:[(256, 64, 1, 1)].\n",
      "    val:[ 3.798e-05  1.875e-03 -2.079e-04 -7.436e-04  2.426e-03]\n",
      "[10] name:[layer1.0.bn3.weight] shape:[(256,)].\n",
      "    val:[ 1.546e-02  1.968e-01  1.069e-07 -3.675e-03 -3.323e-02]\n",
      "[11] name:[layer1.0.bn3.bias] shape:[(256,)].\n",
      "    val:[-4.596e-03  8.297e-02 -2.739e-07  1.699e-01 -1.268e-01]\n",
      "[12] name:[layer1.0.downsample.0.weight] shape:[(256, 64, 1, 1)].\n",
      "    val:[-3.834e-04 -1.242e-04 -5.400e-04 -3.325e-04  3.438e-06]\n",
      "[13] name:[layer1.0.downsample.1.weight] shape:[(256,)].\n",
      "    val:[2.534e-03 2.557e-01 8.880e-08 1.879e-01 1.977e-01]\n",
      "[14] name:[layer1.0.downsample.1.bias] shape:[(256,)].\n",
      "    val:[-4.596e-03  8.297e-02 -2.739e-07  1.699e-01 -1.268e-01]\n",
      "[15] name:[layer1.1.conv1.weight] shape:[(64, 256, 1, 1)].\n",
      "    val:[-1.306e-08  4.598e-09  1.672e-08 -9.112e-09 -2.218e-08]\n",
      "[16] name:[layer1.1.bn1.weight] shape:[(64,)].\n",
      "    val:[4.243e-08 1.474e-01 8.483e-02 1.376e-08 1.652e-06]\n",
      "[17] name:[layer1.1.bn1.bias] shape:[(64,)].\n",
      "    val:[-1.917e-07 -1.323e-01  9.912e-02 -8.398e-08 -1.438e-05]\n",
      "[18] name:[layer1.1.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 6.158e-09 -3.788e-09  5.176e-09 -6.968e-10  8.699e-09]\n",
      "[19] name:[layer1.1.bn2.weight] shape:[(64,)].\n",
      "    val:[1.829e-06 1.548e-01 2.713e-01 1.743e-01 3.104e-08]\n",
      "[20] name:[layer1.1.bn2.bias] shape:[(64,)].\n",
      "    val:[-7.724e-06 -1.693e-02 -3.220e-01 -1.207e-01 -1.206e-07]\n",
      "[21] name:[layer1.1.conv3.weight] shape:[(256, 64, 1, 1)].\n",
      "    val:[-6.623e-08 -3.080e-05 -1.826e-06  2.628e-05 -2.235e-08]\n",
      "[22] name:[layer1.1.bn3.weight] shape:[(256,)].\n",
      "    val:[ 8.816e-05  1.885e-03 -1.336e-05  2.004e-03  2.507e-03]\n",
      "[23] name:[layer1.1.bn3.bias] shape:[(256,)].\n",
      "    val:[-0.004  0.004 -0.     0.005 -0.031]\n",
      "[24] name:[layer1.2.conv1.weight] shape:[(64, 256, 1, 1)].\n",
      "    val:[ 7.929e-04 -2.732e-02 -9.925e-07 -6.554e-03 -1.159e-02]\n",
      "[25] name:[layer1.2.bn1.weight] shape:[(64,)].\n",
      "    val:[0.233 0.089 0.147 0.155 0.169]\n",
      "[26] name:[layer1.2.bn1.bias] shape:[(64,)].\n",
      "    val:[ 0.02  -0.081  0.056  0.003  0.283]\n",
      "[27] name:[layer1.2.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[-0.109  0.114 -0.039 -0.195  0.318]\n",
      "[28] name:[layer1.2.bn2.weight] shape:[(64,)].\n",
      "    val:[0.179 0.126 0.228 0.15  0.168]\n",
      "[29] name:[layer1.2.bn2.bias] shape:[(64,)].\n",
      "    val:[-0.136  0.039 -0.14  -0.039  0.017]\n",
      "[30] name:[layer1.2.conv3.weight] shape:[(256, 64, 1, 1)].\n",
      "    val:[-2.251e-05  1.249e-05  2.650e-05 -6.449e-06 -1.633e-05]\n",
      "[31] name:[layer1.2.bn3.weight] shape:[(256,)].\n",
      "    val:[ 9.646e-06  1.379e-02  1.431e-01 -3.453e-03 -4.229e-03]\n",
      "[32] name:[layer1.2.bn3.bias] shape:[(256,)].\n",
      "    val:[ 0.     0.007  0.096  0.003 -0.018]\n",
      "[33] name:[layer2.0.conv1.weight] shape:[(128, 256, 1, 1)].\n",
      "    val:[-0.    -0.007 -0.    -0.022 -0.001]\n",
      "[34] name:[layer2.0.bn1.weight] shape:[(128,)].\n",
      "    val:[0.137 0.114 0.103 0.154 0.074]\n",
      "[35] name:[layer2.0.bn1.bias] shape:[(128,)].\n",
      "    val:[-0.119 -0.068 -0.026  0.04  -0.043]\n",
      "[36] name:[layer2.0.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[ 0.008 -0.006 -0.009 -0.     0.002]\n",
      "[37] name:[layer2.0.bn2.weight] shape:[(128,)].\n",
      "    val:[0.16  0.148 0.2   0.186 0.169]\n",
      "[38] name:[layer2.0.bn2.bias] shape:[(128,)].\n",
      "    val:[ 0.118  0.153 -0.051  0.008  0.035]\n",
      "[39] name:[layer2.0.conv3.weight] shape:[(512, 128, 1, 1)].\n",
      "    val:[5.742e-03 2.109e-03 7.338e-04 9.359e-04 4.875e-05]\n",
      "[40] name:[layer2.0.bn3.weight] shape:[(512,)].\n",
      "    val:[-0.001  0.264  0.245  0.003  0.001]\n",
      "[41] name:[layer2.0.bn3.bias] shape:[(512,)].\n",
      "    val:[ 0.06   0.003 -0.003  0.221 -0.001]\n",
      "[42] name:[layer2.0.downsample.0.weight] shape:[(512, 256, 1, 1)].\n",
      "    val:[ 0.    -0.039 -0.006 -0.099  0.011]\n",
      "[43] name:[layer2.0.downsample.1.weight] shape:[(512,)].\n",
      "    val:[ 1.054e-01  2.509e-02  4.323e-03  2.087e-01 -4.241e-05]\n",
      "[44] name:[layer2.0.downsample.1.bias] shape:[(512,)].\n",
      "    val:[ 0.06   0.003 -0.003  0.221 -0.001]\n",
      "[45] name:[layer2.1.conv1.weight] shape:[(128, 512, 1, 1)].\n",
      "    val:[-4.185e-03  5.201e-03 -6.130e-03 -6.673e-03  6.201e-05]\n",
      "[46] name:[layer2.1.bn1.weight] shape:[(128,)].\n",
      "    val:[0.113 0.12  0.115 0.103 0.083]\n",
      "[47] name:[layer2.1.bn1.bias] shape:[(128,)].\n",
      "    val:[-0.073 -0.035 -0.001  0.189 -0.036]\n",
      "[48] name:[layer2.1.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[0.006 0.013 0.01  0.005 0.009]\n",
      "[49] name:[layer2.1.bn2.weight] shape:[(128,)].\n",
      "    val:[0.071 0.12  0.059 0.158 0.172]\n",
      "[50] name:[layer2.1.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.022 -0.039  0.018 -0.15   0.129]\n",
      "[51] name:[layer2.1.conv3.weight] shape:[(512, 128, 1, 1)].\n",
      "    val:[ 0.007  0.025  0.011 -0.014  0.056]\n",
      "[52] name:[layer2.1.bn3.weight] shape:[(512,)].\n",
      "    val:[ 0.183  0.001  0.005  0.183 -0.001]\n",
      "[53] name:[layer2.1.bn3.bias] shape:[(512,)].\n",
      "    val:[-0.026  0.005  0.009  0.055 -0.002]\n",
      "[54] name:[layer2.2.conv1.weight] shape:[(128, 512, 1, 1)].\n",
      "    val:[ 1.314e-02 -1.147e-02 -2.375e-02 -6.966e-03 -2.239e-05]\n",
      "[55] name:[layer2.2.bn1.weight] shape:[(128,)].\n",
      "    val:[0.154 0.158 0.195 0.149 0.171]\n",
      "[56] name:[layer2.2.bn1.bias] shape:[(128,)].\n",
      "    val:[-0.094 -0.081 -0.039 -0.013  0.002]\n",
      "[57] name:[layer2.2.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[ 0.003  0.003  0.001 -0.006  0.005]\n",
      "[58] name:[layer2.2.bn2.weight] shape:[(128,)].\n",
      "    val:[0.197 0.152 0.167 0.196 0.145]\n",
      "[59] name:[layer2.2.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.095  0.014 -0.13  -0.085 -0.031]\n",
      "[60] name:[layer2.2.conv3.weight] shape:[(512, 128, 1, 1)].\n",
      "    val:[ 0.002 -0.002  0.001  0.001  0.001]\n",
      "[61] name:[layer2.2.bn3.weight] shape:[(512,)].\n",
      "    val:[-0.003  0.011  0.008  0.139  0.247]\n",
      "[62] name:[layer2.2.bn3.bias] shape:[(512,)].\n",
      "    val:[ 4.443e-03  4.137e-03 -8.250e-05  5.175e-02  1.072e-02]\n",
      "[63] name:[layer2.3.conv1.weight] shape:[(128, 512, 1, 1)].\n",
      "    val:[ 0.    -0.005  0.009 -0.002 -0.003]\n",
      "[64] name:[layer2.3.bn1.weight] shape:[(128,)].\n",
      "    val:[0.144 0.138 0.132 0.125 0.155]\n",
      "[65] name:[layer2.3.bn1.bias] shape:[(128,)].\n",
      "    val:[-0.028  0.001  0.039  0.025 -0.098]\n",
      "[66] name:[layer2.3.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.013 -0.013  0.003  0.035  0.064]\n",
      "[67] name:[layer2.3.bn2.weight] shape:[(128,)].\n",
      "    val:[0.197 0.17  0.135 0.203 0.151]\n",
      "[68] name:[layer2.3.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.051 -0.02   0.13  -0.094 -0.035]\n",
      "[69] name:[layer2.3.conv3.weight] shape:[(512, 128, 1, 1)].\n",
      "    val:[-0.002 -0.003 -0.003  0.003  0.007]\n",
      "[70] name:[layer2.3.bn3.weight] shape:[(512,)].\n",
      "    val:[ 0.008  0.068  0.147 -0.     0.026]\n",
      "[71] name:[layer2.3.bn3.bias] shape:[(512,)].\n",
      "    val:[ 0.002 -0.038 -0.139  0.    -0.029]\n",
      "[72] name:[layer3.0.conv1.weight] shape:[(256, 512, 1, 1)].\n",
      "    val:[ 0.001  0.015  0.008 -0.011  0.028]\n",
      "[73] name:[layer3.0.bn1.weight] shape:[(256,)].\n",
      "    val:[0.225 0.198 0.175 0.194 0.22 ]\n",
      "[74] name:[layer3.0.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.14  -0.064 -0.096 -0.112 -0.048]\n",
      "[75] name:[layer3.0.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.009 -0.014 -0.025 -0.009  0.002]\n",
      "[76] name:[layer3.0.bn2.weight] shape:[(256,)].\n",
      "    val:[0.177 0.142 0.3   0.193 0.158]\n",
      "[77] name:[layer3.0.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.021  0.18  -0.149 -0.052  0.137]\n",
      "[78] name:[layer3.0.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[ 3.718e-08  1.051e-07 -1.152e-08 -7.354e-08 -8.387e-08]\n",
      "[79] name:[layer3.0.bn3.weight] shape:[(1024,)].\n",
      "    val:[-2.543e-08 -1.072e-02  2.179e-01  1.946e-01  2.329e-01]\n",
      "[80] name:[layer3.0.bn3.bias] shape:[(1024,)].\n",
      "    val:[-3.652e-07  2.508e-02 -3.019e-02 -1.212e-03  2.226e-02]\n",
      "[81] name:[layer3.0.downsample.0.weight] shape:[(1024, 512, 1, 1)].\n",
      "    val:[-3.483e-08 -6.623e-08 -5.924e-08  3.392e-08 -7.617e-08]\n",
      "[82] name:[layer3.0.downsample.1.weight] shape:[(1024,)].\n",
      "    val:[1.668e-07 7.099e-02 1.061e-01 9.647e-02 7.168e-02]\n",
      "[83] name:[layer3.0.downsample.1.bias] shape:[(1024,)].\n",
      "    val:[-3.652e-07  2.508e-02 -3.019e-02 -1.212e-03  2.226e-02]\n",
      "[84] name:[layer3.1.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[ 7.568e-09 -5.230e-04  4.310e-03 -7.656e-04 -7.249e-04]\n",
      "[85] name:[layer3.1.bn1.weight] shape:[(256,)].\n",
      "    val:[0.034 0.034 0.054 0.08  0.052]\n",
      "[86] name:[layer3.1.bn1.bias] shape:[(256,)].\n",
      "    val:[ 0.009  0.01  -0.019 -0.056 -0.008]\n",
      "[87] name:[layer3.1.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.     0.002 -0.002 -0.003 -0.   ]\n",
      "[88] name:[layer3.1.bn2.weight] shape:[(256,)].\n",
      "    val:[0.049 0.031 0.081 0.08  0.075]\n",
      "[89] name:[layer3.1.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.009 -0.013 -0.055 -0.003 -0.031]\n",
      "[90] name:[layer3.1.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.004 -0.002  0.    -0.003 -0.003]\n",
      "[91] name:[layer3.1.bn3.weight] shape:[(1024,)].\n",
      "    val:[ 0.075 -0.    -0.004 -0.003  0.002]\n",
      "[92] name:[layer3.1.bn3.bias] shape:[(1024,)].\n",
      "    val:[ 0.03   0.004 -0.021  0.011  0.01 ]\n",
      "[93] name:[layer3.2.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[-0.001 -0.004 -0.003 -0.002 -0.   ]\n",
      "[94] name:[layer3.2.bn1.weight] shape:[(256,)].\n",
      "    val:[0.037 0.102 0.077 0.087 0.087]\n",
      "[95] name:[layer3.2.bn1.bias] shape:[(256,)].\n",
      "    val:[ 0.025 -0.008  0.02   0.019 -0.044]\n",
      "[96] name:[layer3.2.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[0.002 0.002 0.003 0.002 0.002]\n",
      "[97] name:[layer3.2.bn2.weight] shape:[(256,)].\n",
      "    val:[0.113 0.16  0.11  0.134 0.148]\n",
      "[98] name:[layer3.2.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.024 -0.111 -0.071 -0.092 -0.076]\n",
      "[99] name:[layer3.2.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[ 0.003 -0.001  0.003 -0.004  0.007]\n",
      "[100] name:[layer3.2.bn3.weight] shape:[(1024,)].\n",
      "    val:[ 0.022  0.016  0.068  0.073 -0.001]\n",
      "[101] name:[layer3.2.bn3.bias] shape:[(1024,)].\n",
      "    val:[ 0.02  -0.016 -0.021 -0.065  0.008]\n",
      "[102] name:[layer3.3.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[ 0.    -0.004 -0.002  0.004 -0.004]\n",
      "[103] name:[layer3.3.bn1.weight] shape:[(256,)].\n",
      "    val:[0.043 0.047 0.086 0.105 0.069]\n",
      "[104] name:[layer3.3.bn1.bias] shape:[(256,)].\n",
      "    val:[-5.654e-05 -6.844e-03 -4.452e-02 -1.603e-02 -3.145e-02]\n",
      "[105] name:[layer3.3.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[0.004 0.003 0.    0.003 0.001]\n",
      "[106] name:[layer3.3.bn2.weight] shape:[(256,)].\n",
      "    val:[0.087 0.107 0.097 0.084 0.09 ]\n",
      "[107] name:[layer3.3.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.066 -0.054 -0.033 -0.06  -0.058]\n",
      "[108] name:[layer3.3.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.009 -0.002  0.008  0.026 -0.001]\n",
      "[109] name:[layer3.3.bn3.weight] shape:[(1024,)].\n",
      "    val:[ 0.086  0.003 -0.022  0.013 -0.011]\n",
      "[110] name:[layer3.3.bn3.bias] shape:[(1024,)].\n",
      "    val:[ 0.024 -0.023 -0.018  0.033  0.01 ]\n",
      "[111] name:[layer3.4.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[ 0.003  0.003 -0.007 -0.02   0.017]\n",
      "[112] name:[layer3.4.bn1.weight] shape:[(256,)].\n",
      "    val:[0.117 0.091 0.105 0.078 0.093]\n",
      "[113] name:[layer3.4.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.041 -0.068 -0.05  -0.019 -0.056]\n",
      "[114] name:[layer3.4.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.002 -0.004 -0.001  0.007 -0.002]\n",
      "[115] name:[layer3.4.bn2.weight] shape:[(256,)].\n",
      "    val:[0.1   0.102 0.095 0.109 0.099]\n",
      "[116] name:[layer3.4.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.075  0.02  -0.037 -0.057  0.057]\n",
      "[117] name:[layer3.4.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[ 0.     0.002 -0.002 -0.     0.002]\n",
      "[118] name:[layer3.4.bn3.weight] shape:[(1024,)].\n",
      "    val:[3.047e-03 6.041e-03 2.287e-02 2.059e-05 1.076e-03]\n",
      "[119] name:[layer3.4.bn3.bias] shape:[(1024,)].\n",
      "    val:[ 0.003 -0.018 -0.021  0.032  0.011]\n",
      "[120] name:[layer3.5.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[ 0.003 -0.004  0.006 -0.017  0.001]\n",
      "[121] name:[layer3.5.bn1.weight] shape:[(256,)].\n",
      "    val:[0.12  0.116 0.073 0.116 0.14 ]\n",
      "[122] name:[layer3.5.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.075 -0.038  0.003 -0.065 -0.099]\n",
      "[123] name:[layer3.5.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.02  -0.015 -0.016 -0.013  0.   ]\n",
      "[124] name:[layer3.5.bn2.weight] shape:[(256,)].\n",
      "    val:[0.123 0.14  0.129 0.092 0.111]\n",
      "[125] name:[layer3.5.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.074 -0.079 -0.088  0.    -0.049]\n",
      "[126] name:[layer3.5.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[ 3.237e-03 -4.272e-03  5.219e-03 -4.797e-05 -5.592e-03]\n",
      "[127] name:[layer3.5.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.02  0.034 0.057 0.048 0.006]\n",
      "[128] name:[layer3.5.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.038 -0.015 -0.023  0.033  0.011]\n",
      "[129] name:[layer3.6.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[-0.014 -0.002 -0.007 -0.002 -0.008]\n",
      "[130] name:[layer3.6.bn1.weight] shape:[(256,)].\n",
      "    val:[0.102 0.12  0.117 0.113 0.113]\n",
      "[131] name:[layer3.6.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.07  -0.106 -0.093 -0.051 -0.09 ]\n",
      "[132] name:[layer3.6.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.008  0.     0.006 -0.007  0.001]\n",
      "[133] name:[layer3.6.bn2.weight] shape:[(256,)].\n",
      "    val:[0.115 0.079 0.105 0.08  0.11 ]\n",
      "[134] name:[layer3.6.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.092  0.012 -0.051 -0.015 -0.046]\n",
      "[135] name:[layer3.6.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.002 -0.006  0.002 -0.002 -0.007]\n",
      "[136] name:[layer3.6.bn3.weight] shape:[(1024,)].\n",
      "    val:[-0.019  0.024  0.041  0.091  0.002]\n",
      "[137] name:[layer3.6.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.046 -0.015 -0.026  0.034  0.01 ]\n",
      "[138] name:[layer3.7.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[-0.001  0.    -0.006  0.017 -0.005]\n",
      "[139] name:[layer3.7.bn1.weight] shape:[(256,)].\n",
      "    val:[0.063 0.136 0.092 0.095 0.119]\n",
      "[140] name:[layer3.7.bn1.bias] shape:[(256,)].\n",
      "    val:[ 0.065 -0.08  -0.002 -0.074 -0.022]\n",
      "[141] name:[layer3.7.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.008  0.008  0.001  0.001 -0.001]\n",
      "[142] name:[layer3.7.bn2.weight] shape:[(256,)].\n",
      "    val:[0.168 0.134 0.113 0.114 0.147]\n",
      "[143] name:[layer3.7.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.104 -0.077 -0.067 -0.046 -0.089]\n",
      "[144] name:[layer3.7.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.001 -0.002  0.001  0.004 -0.003]\n",
      "[145] name:[layer3.7.bn3.weight] shape:[(1024,)].\n",
      "    val:[-0.011 -0.005  0.075  0.067  0.014]\n",
      "[146] name:[layer3.7.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.043 -0.013 -0.028  0.018  0.01 ]\n",
      "[147] name:[layer3.8.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[-0.005  0.     0.006 -0.004  0.001]\n",
      "[148] name:[layer3.8.bn1.weight] shape:[(256,)].\n",
      "    val:[0.085 0.122 0.146 0.118 0.134]\n",
      "[149] name:[layer3.8.bn1.bias] shape:[(256,)].\n",
      "    val:[ 0.007 -0.08  -0.095 -0.097 -0.07 ]\n",
      "[150] name:[layer3.8.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.007 -0.007 -0.007 -0.021 -0.005]\n",
      "[151] name:[layer3.8.bn2.weight] shape:[(256,)].\n",
      "    val:[0.122 0.129 0.106 0.135 0.139]\n",
      "[152] name:[layer3.8.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.057 -0.054 -0.067 -0.071 -0.095]\n",
      "[153] name:[layer3.8.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.    -0.001  0.001  0.001  0.   ]\n",
      "[154] name:[layer3.8.bn3.weight] shape:[(1024,)].\n",
      "    val:[-0.001  0.015  0.067  0.056  0.016]\n",
      "[155] name:[layer3.8.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.044 -0.012 -0.038 -0.003  0.009]\n",
      "[156] name:[layer3.9.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[-0.005 -0.001 -0.015  0.01   0.004]\n",
      "[157] name:[layer3.9.bn1.weight] shape:[(256,)].\n",
      "    val:[0.124 0.125 0.12  0.126 0.09 ]\n",
      "[158] name:[layer3.9.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.07  -0.035 -0.058 -0.044  0.04 ]\n",
      "[159] name:[layer3.9.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.011 -0.026 -0.02   0.007  0.03 ]\n",
      "[160] name:[layer3.9.bn2.weight] shape:[(256,)].\n",
      "    val:[0.151 0.14  0.128 0.126 0.159]\n",
      "[161] name:[layer3.9.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.119 -0.115 -0.049 -0.039 -0.072]\n",
      "[162] name:[layer3.9.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[ 0.012  0.007 -0.001 -0.008 -0.007]\n",
      "[163] name:[layer3.9.bn3.weight] shape:[(1024,)].\n",
      "    val:[ 0.06   0.007  0.112 -0.006  0.   ]\n",
      "[164] name:[layer3.9.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.015 -0.013 -0.001  0.005  0.004]\n",
      "[165] name:[layer3.10.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[-0.001 -0.     0.002 -0.012 -0.002]\n",
      "[166] name:[layer3.10.bn1.weight] shape:[(256,)].\n",
      "    val:[0.136 0.118 0.1   0.126 0.092]\n",
      "[167] name:[layer3.10.bn1.bias] shape:[(256,)].\n",
      "    val:[-7.900e-02 -3.106e-05 -2.434e-02 -5.533e-02  1.054e-02]\n",
      "[168] name:[layer3.10.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.009  0.025  0.001 -0.023 -0.009]\n",
      "[169] name:[layer3.10.bn2.weight] shape:[(256,)].\n",
      "    val:[0.144 0.144 0.164 0.131 0.152]\n",
      "[170] name:[layer3.10.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.094 -0.103 -0.089 -0.036 -0.04 ]\n",
      "[171] name:[layer3.10.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.004  0.001 -0.001 -0.024 -0.001]\n",
      "[172] name:[layer3.10.bn3.weight] shape:[(1024,)].\n",
      "    val:[ 0.146  0.007  0.162 -0.007  0.119]\n",
      "[173] name:[layer3.10.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.018 -0.013 -0.056  0.003 -0.01 ]\n",
      "[174] name:[layer3.11.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[-7.159e-03 -9.064e-06  3.155e-03 -4.180e-03 -1.094e-02]\n",
      "[175] name:[layer3.11.bn1.weight] shape:[(256,)].\n",
      "    val:[0.119 0.114 0.107 0.122 0.122]\n",
      "[176] name:[layer3.11.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.042 -0.078 -0.015 -0.062 -0.053]\n",
      "[177] name:[layer3.11.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.005  0.004 -0.011  0.04   0.007]\n",
      "[178] name:[layer3.11.bn2.weight] shape:[(256,)].\n",
      "    val:[0.138 0.165 0.152 0.142 0.13 ]\n",
      "[179] name:[layer3.11.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.046 -0.117 -0.113 -0.085 -0.035]\n",
      "[180] name:[layer3.11.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.012 -0.018  0.016 -0.017 -0.013]\n",
      "[181] name:[layer3.11.bn3.weight] shape:[(1024,)].\n",
      "    val:[ 0.113  0.059  0.105 -0.015  0.215]\n",
      "[182] name:[layer3.11.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.112  0.031 -0.081  0.005 -0.089]\n",
      "[183] name:[layer3.12.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[ 1.499e-05 -1.147e-04  3.317e-03  1.796e-02  2.284e-02]\n",
      "[184] name:[layer3.12.bn1.weight] shape:[(256,)].\n",
      "    val:[0.104 0.114 0.103 0.116 0.12 ]\n",
      "[185] name:[layer3.12.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.058 -0.084 -0.038 -0.053 -0.059]\n",
      "[186] name:[layer3.12.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.007 -0.014 -0.01   0.01  -0.   ]\n",
      "[187] name:[layer3.12.bn2.weight] shape:[(256,)].\n",
      "    val:[0.08  0.135 0.144 0.14  0.064]\n",
      "[188] name:[layer3.12.bn2.bias] shape:[(256,)].\n",
      "    val:[ 0.017 -0.099 -0.107 -0.072  0.054]\n",
      "[189] name:[layer3.12.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.018 -0.013 -0.008 -0.015 -0.023]\n",
      "[190] name:[layer3.12.bn3.weight] shape:[(1024,)].\n",
      "    val:[1.022e-01 6.669e-02 5.523e-02 6.338e-05 7.345e-02]\n",
      "[191] name:[layer3.12.bn3.bias] shape:[(1024,)].\n",
      "    val:[ 0.014  0.004 -0.073  0.005 -0.071]\n",
      "[192] name:[layer3.13.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[ 0.007 -0.003  0.006 -0.024  0.005]\n",
      "[193] name:[layer3.13.bn1.weight] shape:[(256,)].\n",
      "    val:[0.102 0.122 0.161 0.126 0.14 ]\n",
      "[194] name:[layer3.13.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.026 -0.08  -0.157 -0.095 -0.104]\n",
      "[195] name:[layer3.13.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.022 -0.012  0.003 -0.009 -0.013]\n",
      "[196] name:[layer3.13.bn2.weight] shape:[(256,)].\n",
      "    val:[0.142 0.138 0.166 0.11  0.109]\n",
      "[197] name:[layer3.13.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.114 -0.085 -0.147 -0.007 -0.068]\n",
      "[198] name:[layer3.13.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[ 0.006  0.003 -0.021  0.007  0.   ]\n",
      "[199] name:[layer3.13.bn3.weight] shape:[(1024,)].\n",
      "    val:[ 0.061  0.053  0.05  -0.002  0.056]\n",
      "[200] name:[layer3.13.bn3.bias] shape:[(1024,)].\n",
      "    val:[ 0.001  0.002 -0.06   0.005 -0.046]\n",
      "[201] name:[layer3.14.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[ 0.     0.004 -0.001 -0.006 -0.006]\n",
      "[202] name:[layer3.14.bn1.weight] shape:[(256,)].\n",
      "    val:[0.182 0.092 0.125 0.142 0.114]\n",
      "[203] name:[layer3.14.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.131 -0.01  -0.082 -0.058 -0.047]\n",
      "[204] name:[layer3.14.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.005 -0.005 -0.     0.005  0.006]\n",
      "[205] name:[layer3.14.bn2.weight] shape:[(256,)].\n",
      "    val:[0.142 0.103 0.119 0.11  0.135]\n",
      "[206] name:[layer3.14.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.087 -0.023 -0.048 -0.042 -0.091]\n",
      "[207] name:[layer3.14.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-6.224e-05 -2.312e-03 -4.852e-03  5.350e-03 -2.271e-03]\n",
      "[208] name:[layer3.14.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.071 0.092 0.073 0.023 0.07 ]\n",
      "[209] name:[layer3.14.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.035  0.006 -0.03   0.006 -0.044]\n",
      "[210] name:[layer3.15.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[-0.01  -0.027 -0.014  0.004  0.009]\n",
      "[211] name:[layer3.15.bn1.weight] shape:[(256,)].\n",
      "    val:[0.089 0.114 0.139 0.135 0.088]\n",
      "[212] name:[layer3.15.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.011 -0.06  -0.067 -0.08  -0.008]\n",
      "[213] name:[layer3.15.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.029 -0.035 -0.03  -0.001 -0.005]\n",
      "[214] name:[layer3.15.bn2.weight] shape:[(256,)].\n",
      "    val:[0.09  0.123 0.084 0.122 0.124]\n",
      "[215] name:[layer3.15.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.015 -0.062  0.023 -0.107 -0.094]\n",
      "[216] name:[layer3.15.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[ 0.004 -0.002 -0.022 -0.013 -0.012]\n",
      "[217] name:[layer3.15.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.067 0.104 0.059 0.005 0.034]\n",
      "[218] name:[layer3.15.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.025 -0.026  0.018  0.005 -0.037]\n",
      "[219] name:[layer3.16.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[-0.004 -0.004  0.003 -0.019 -0.005]\n",
      "[220] name:[layer3.16.bn1.weight] shape:[(256,)].\n",
      "    val:[0.093 0.16  0.122 0.095 0.134]\n",
      "[221] name:[layer3.16.bn1.bias] shape:[(256,)].\n",
      "    val:[ 0.013 -0.104 -0.109 -0.029 -0.06 ]\n",
      "[222] name:[layer3.16.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.002  0.003 -0.018  0.009  0.004]\n",
      "[223] name:[layer3.16.bn2.weight] shape:[(256,)].\n",
      "    val:[0.116 0.088 0.121 0.115 0.136]\n",
      "[224] name:[layer3.16.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.053 -0.004 -0.085 -0.06  -0.087]\n",
      "[225] name:[layer3.16.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[ 0.001 -0.001 -0.004 -0.008 -0.004]\n",
      "[226] name:[layer3.16.bn3.weight] shape:[(1024,)].\n",
      "    val:[ 0.009  0.081  0.05  -0.001  0.044]\n",
      "[227] name:[layer3.16.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.056 -0.03   0.002  0.005 -0.031]\n",
      "[228] name:[layer3.17.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[ 0.019  0.006  0.    -0.     0.003]\n",
      "[229] name:[layer3.17.bn1.weight] shape:[(256,)].\n",
      "    val:[0.12  0.135 0.157 0.08  0.121]\n",
      "[230] name:[layer3.17.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.033 -0.109 -0.133  0.026 -0.066]\n",
      "[231] name:[layer3.17.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.009 -0.008 -0.01  -0.003 -0.015]\n",
      "[232] name:[layer3.17.bn2.weight] shape:[(256,)].\n",
      "    val:[0.12  0.131 0.123 0.129 0.176]\n",
      "[233] name:[layer3.17.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.065 -0.113 -0.053 -0.067 -0.103]\n",
      "[234] name:[layer3.17.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.014 -0.001  0.011 -0.006 -0.022]\n",
      "[235] name:[layer3.17.bn3.weight] shape:[(1024,)].\n",
      "    val:[ 0.151  0.096  0.068 -0.001  0.043]\n",
      "[236] name:[layer3.17.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.005 -0.049 -0.019  0.006 -0.037]\n",
      "[237] name:[layer3.18.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[ 0.002 -0.006  0.003 -0.001 -0.011]\n",
      "[238] name:[layer3.18.bn1.weight] shape:[(256,)].\n",
      "    val:[0.118 0.127 0.14  0.148 0.139]\n",
      "[239] name:[layer3.18.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.082 -0.059 -0.109 -0.08  -0.063]\n",
      "[240] name:[layer3.18.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.003 -0.008 -0.008  0.002 -0.012]\n",
      "[241] name:[layer3.18.bn2.weight] shape:[(256,)].\n",
      "    val:[0.147 0.138 0.148 0.137 0.144]\n",
      "[242] name:[layer3.18.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.106 -0.105 -0.105 -0.077 -0.104]\n",
      "[243] name:[layer3.18.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.016  0.015  0.017  0.029 -0.001]\n",
      "[244] name:[layer3.18.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.087 0.09  0.057 0.009 0.046]\n",
      "[245] name:[layer3.18.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.044 -0.073 -0.066  0.006 -0.032]\n",
      "[246] name:[layer3.19.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[-0.026  0.006 -0.016  0.017 -0.001]\n",
      "[247] name:[layer3.19.bn1.weight] shape:[(256,)].\n",
      "    val:[0.092 0.128 0.117 0.154 0.121]\n",
      "[248] name:[layer3.19.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.003 -0.074 -0.032 -0.113 -0.068]\n",
      "[249] name:[layer3.19.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.006 -0.015 -0.019  0.007 -0.011]\n",
      "[250] name:[layer3.19.bn2.weight] shape:[(256,)].\n",
      "    val:[0.125 0.146 0.132 0.156 0.142]\n",
      "[251] name:[layer3.19.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.074 -0.073 -0.123 -0.136 -0.063]\n",
      "[252] name:[layer3.19.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[ 0.009  0.004 -0.003 -0.012 -0.019]\n",
      "[253] name:[layer3.19.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.067 0.061 0.073 0.016 0.051]\n",
      "[254] name:[layer3.19.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.063 -0.041 -0.07   0.001 -0.026]\n",
      "[255] name:[layer3.20.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[ 0.008 -0.015  0.008 -0.041  0.005]\n",
      "[256] name:[layer3.20.bn1.weight] shape:[(256,)].\n",
      "    val:[0.143 0.166 0.162 0.159 0.164]\n",
      "[257] name:[layer3.20.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.116 -0.148 -0.136 -0.122 -0.161]\n",
      "[258] name:[layer3.20.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.008 -0.002  0.01  -0.007 -0.005]\n",
      "[259] name:[layer3.20.bn2.weight] shape:[(256,)].\n",
      "    val:[0.159 0.151 0.162 0.136 0.127]\n",
      "[260] name:[layer3.20.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.135 -0.084 -0.113 -0.056 -0.062]\n",
      "[261] name:[layer3.20.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[ 0.012 -0.004 -0.011  0.015  0.005]\n",
      "[262] name:[layer3.20.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.071 0.057 0.089 0.024 0.044]\n",
      "[263] name:[layer3.20.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.08  -0.029 -0.085 -0.011 -0.04 ]\n",
      "[264] name:[layer3.21.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[ 0.041 -0.023 -0.002 -0.006 -0.006]\n",
      "[265] name:[layer3.21.bn1.weight] shape:[(256,)].\n",
      "    val:[0.129 0.136 0.142 0.153 0.175]\n",
      "[266] name:[layer3.21.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.121 -0.071 -0.107 -0.128 -0.173]\n",
      "[267] name:[layer3.21.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.007 -0.014  0.002  0.017 -0.003]\n",
      "[268] name:[layer3.21.bn2.weight] shape:[(256,)].\n",
      "    val:[0.159 0.141 0.151 0.161 0.135]\n",
      "[269] name:[layer3.21.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.108 -0.088 -0.074 -0.103 -0.091]\n",
      "[270] name:[layer3.21.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[ 0.008  0.006  0.013 -0.004  0.001]\n",
      "[271] name:[layer3.21.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.064 0.059 0.183 0.058 0.067]\n",
      "[272] name:[layer3.21.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.074  0.03  -0.13  -0.022 -0.067]\n",
      "[273] name:[layer3.22.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[ 0.014 -0.032 -0.024 -0.01  -0.017]\n",
      "[274] name:[layer3.22.bn1.weight] shape:[(256,)].\n",
      "    val:[0.188 0.203 0.222 0.19  0.167]\n",
      "[275] name:[layer3.22.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.126 -0.174 -0.268 -0.109 -0.142]\n",
      "[276] name:[layer3.22.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[0.005 0.011 0.009 0.007 0.009]\n",
      "[277] name:[layer3.22.bn2.weight] shape:[(256,)].\n",
      "    val:[0.149 0.185 0.16  0.152 0.389]\n",
      "[278] name:[layer3.22.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.067 -0.151 -0.064 -0.057 -0.196]\n",
      "[279] name:[layer3.22.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.019 -0.017 -0.025 -0.019 -0.024]\n",
      "[280] name:[layer3.22.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.108 0.079 0.169 0.046 0.08 ]\n",
      "[281] name:[layer3.22.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.048 -0.034 -0.151  0.007 -0.006]\n",
      "[282] name:[layer4.0.conv1.weight] shape:[(512, 1024, 1, 1)].\n",
      "    val:[ 0.013 -0.034  0.013  0.009 -0.005]\n",
      "[283] name:[layer4.0.bn1.weight] shape:[(512,)].\n",
      "    val:[0.174 0.204 0.22  0.197 0.225]\n",
      "[284] name:[layer4.0.bn1.bias] shape:[(512,)].\n",
      "    val:[-0.159 -0.163 -0.209 -0.164 -0.203]\n",
      "[285] name:[layer4.0.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[-0.007 -0.018 -0.003 -0.013 -0.016]\n",
      "[286] name:[layer4.0.bn2.weight] shape:[(512,)].\n",
      "    val:[0.168 0.159 0.184 0.163 0.22 ]\n",
      "[287] name:[layer4.0.bn2.bias] shape:[(512,)].\n",
      "    val:[-0.075 -0.096 -0.092 -0.051 -0.128]\n",
      "[288] name:[layer4.0.conv3.weight] shape:[(2048, 512, 1, 1)].\n",
      "    val:[ 0.022 -0.009  0.014 -0.014 -0.004]\n",
      "[289] name:[layer4.0.bn3.weight] shape:[(2048,)].\n",
      "    val:[0.339 0.402 0.483 0.301 0.437]\n",
      "[290] name:[layer4.0.bn3.bias] shape:[(2048,)].\n",
      "    val:[-0.013 -0.049  0.019 -0.004 -0.053]\n",
      "[291] name:[layer4.0.downsample.0.weight] shape:[(2048, 1024, 1, 1)].\n",
      "    val:[ 0.02  -0.005  0.009 -0.002 -0.006]\n",
      "[292] name:[layer4.0.downsample.1.weight] shape:[(2048,)].\n",
      "    val:[0.323 0.305 0.539 0.353 0.302]\n",
      "[293] name:[layer4.0.downsample.1.bias] shape:[(2048,)].\n",
      "    val:[-0.013 -0.049  0.019 -0.004 -0.053]\n",
      "[294] name:[layer4.1.conv1.weight] shape:[(512, 2048, 1, 1)].\n",
      "    val:[ 0.006 -0.024  0.018 -0.008 -0.016]\n",
      "[295] name:[layer4.1.bn1.weight] shape:[(512,)].\n",
      "    val:[0.189 0.192 0.187 0.191 0.168]\n",
      "[296] name:[layer4.1.bn1.bias] shape:[(512,)].\n",
      "    val:[-0.154 -0.172 -0.124 -0.133 -0.106]\n",
      "[297] name:[layer4.1.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[-0.016 -0.012 -0.009 -0.009 -0.007]\n",
      "[298] name:[layer4.1.bn2.weight] shape:[(512,)].\n",
      "    val:[0.2   0.21  0.17  0.193 0.19 ]\n",
      "[299] name:[layer4.1.bn2.bias] shape:[(512,)].\n",
      "    val:[-0.127 -0.118 -0.05  -0.104 -0.088]\n",
      "[300] name:[layer4.1.conv3.weight] shape:[(2048, 512, 1, 1)].\n",
      "    val:[ 0.022 -0.016  0.026  0.005  0.011]\n",
      "[301] name:[layer4.1.bn3.weight] shape:[(2048,)].\n",
      "    val:[0.436 0.334 0.625 0.336 0.429]\n",
      "[302] name:[layer4.1.bn3.bias] shape:[(2048,)].\n",
      "    val:[-0.002 -0.029  0.019 -0.031 -0.039]\n",
      "[303] name:[layer4.2.conv1.weight] shape:[(512, 2048, 1, 1)].\n",
      "    val:[-0.003 -0.009 -0.017  0.002 -0.008]\n",
      "[304] name:[layer4.2.bn1.weight] shape:[(512,)].\n",
      "    val:[0.189 0.226 0.211 0.221 0.187]\n",
      "[305] name:[layer4.2.bn1.bias] shape:[(512,)].\n",
      "    val:[-0.123 -0.239 -0.199 -0.206 -0.13 ]\n",
      "[306] name:[layer4.2.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[-2.424e-03 -9.124e-06  2.512e-03  2.409e-03  1.326e-03]\n",
      "[307] name:[layer4.2.bn2.weight] shape:[(512,)].\n",
      "    val:[0.186 0.175 0.198 0.182 0.232]\n",
      "[308] name:[layer4.2.bn2.bias] shape:[(512,)].\n",
      "    val:[-0.071 -0.039 -0.096 -0.031 -0.114]\n",
      "[309] name:[layer4.2.conv3.weight] shape:[(2048, 512, 1, 1)].\n",
      "    val:[ 0.009 -0.004  0.024  0.002 -0.019]\n",
      "[310] name:[layer4.2.bn3.weight] shape:[(2048,)].\n",
      "    val:[0.631 0.565 0.797 0.4   0.66 ]\n",
      "[311] name:[layer4.2.bn3.bias] shape:[(2048,)].\n",
      "    val:[ 0.028  0.006  0.008 -0.027 -0.009]\n",
      "[312] name:[fc.weight] shape:[(18, 2048)].\n",
      "    val:[ 0.001 -0.032  0.031 -0.    -0.046]\n",
      "[313] name:[fc.bias] shape:[(18,)].\n",
      "    val:[-0.014 -0.013 -0.012  0.016 -0.005]\n",
      "Total number of parameters:[42,537,042].\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "n_param = 0\n",
    "for p_idx, (param_name, param) in enumerate(basemodel_resnet101.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        param_numpy = param.detach().cpu().numpy()\n",
    "        n_param += len(param_numpy.reshape(-1))\n",
    "        print (\"[%d] name:[%s] shape:[%s].\"%(p_idx,param_name,param_numpy.shape))\n",
    "        print (\"    val:%s\"%(param_numpy.reshape(-1)[:5]))\n",
    "print (\"Total number of parameters:[%s].\"%(format(n_param,',d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c067d0e-38c9-4ab8-baba-88cf8dcb8c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[0/30] training loss 0.0929, training accuracy 0.0938\n",
      "epoch[0/30] training loss 0.0917, training accuracy 0.0000\n",
      "epoch[0/30] training loss 0.0791, training accuracy 0.2188\n",
      "epoch[0/30] training loss 0.0738, training accuracy 0.5000\n",
      "epoch[0/30] training loss 0.0633, training accuracy 0.4375\n",
      "epoch[0/30] training loss 0.0677, training accuracy 0.4375\n",
      "epoch[0/30] training loss 0.0667, training accuracy 0.4688\n",
      "epoch[0/30] training loss 0.0468, training accuracy 0.5625\n",
      "epoch[0/30] training loss 0.0481, training accuracy 0.5938\n",
      "epoch[0/30] training loss 0.0504, training accuracy 0.5000\n",
      "epoch[0/30] training loss 0.0357, training accuracy 0.6875\n",
      "epoch[0/30] training loss 0.0432, training accuracy 0.5938\n",
      "epoch[0/30] training loss 0.0260, training accuracy 0.7500\n",
      "epoch[0/30] training loss 0.0331, training accuracy 0.7188\n",
      "epoch[0/30] training loss 0.0246, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0294, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0302, training accuracy 0.6875\n",
      "epoch[0/30] training loss 0.0258, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0256, training accuracy 0.7188\n",
      "epoch[0/30] training loss 0.0229, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0139, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0316, training accuracy 0.6875\n",
      "epoch[0/30] training loss 0.0220, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0183, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0263, training accuracy 0.7188\n",
      "epoch[0/30] training loss 0.0349, training accuracy 0.6875\n",
      "epoch[0/30] training loss 0.0194, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0172, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0159, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0150, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0251, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0162, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0313, training accuracy 0.7188\n",
      "epoch[0/30] training loss 0.0106, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0189, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0146, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0126, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0163, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0125, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0173, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0160, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0212, training accuracy 0.7500\n",
      "epoch[0/30] training loss 0.0197, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0189, training accuracy 0.7188\n",
      "epoch[0/30] training loss 0.0111, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0144, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0167, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0163, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0206, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0189, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0163, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0176, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0110, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0149, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0124, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0136, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0165, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0102, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0136, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0164, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0117, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0104, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0206, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0050, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0179, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0059, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0125, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0120, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0171, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0172, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0118, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0161, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0138, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0085, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0116, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0086, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0113, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0208, training accuracy 0.7188\n",
      "epoch[0/30] training loss 0.0169, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0106, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0114, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0127, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0119, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0188, training accuracy 0.7500\n",
      "epoch[0/30] training loss 0.0214, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0104, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0104, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0125, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0099, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0062, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0156, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0171, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0102, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0176, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0150, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0095, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0133, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0143, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0114, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0107, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0087, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0229, training accuracy 0.7500\n",
      "epoch[0/30] training loss 0.0138, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0133, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0164, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0071, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0154, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0101, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0167, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0139, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0090, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0114, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0175, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0079, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0090, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0133, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0071, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0154, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0074, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0159, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0147, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0124, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0192, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0075, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0121, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0134, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0049, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0162, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0137, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0185, training accuracy 0.7500\n",
      "epoch[0/30] training loss 0.0159, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0115, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0115, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0028, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0141, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0082, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0100, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0132, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0073, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0080, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0140, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0165, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0145, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0083, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0086, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0095, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0165, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0124, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0069, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0132, training accuracy 0.6875\n",
      "epoch[0/30] training loss 0.0101, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0083, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0075, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0149, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0030, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0114, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0149, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0073, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0187, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0114, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0067, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0068, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0076, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0069, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0074, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0052, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0082, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0068, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0057, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0035, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0053, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0081, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0059, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0046, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0073, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0044, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0057, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0128, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0028, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0082, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0147, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0046, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0181, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0053, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0103, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0111, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0078, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0100, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0070, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0040, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0120, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0188, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0049, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0185, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0068, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0040, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0134, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0131, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0095, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0100, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0071, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0094, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0110, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0075, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0039, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0093, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0043, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0141, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0101, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0089, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0062, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0058, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0082, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0165, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0137, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0042, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0084, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0065, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0078, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0050, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0043, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0156, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0065, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0079, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0035, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0017, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0109, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0140, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0125, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0089, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0038, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0157, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0111, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0051, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0098, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0129, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0103, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0109, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0112, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0081, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0048, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0053, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0062, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0070, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0143, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0044, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0088, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0048, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0031, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0070, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0058, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0056, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0068, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0071, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0085, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0101, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0080, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0054, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0078, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0068, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0100, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0083, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0080, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0091, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0111, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0070, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0087, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0132, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0110, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0087, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0107, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0132, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0039, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0095, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0156, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0053, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0058, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0076, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0043, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0093, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0058, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0075, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0120, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0113, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0064, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0057, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0144, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0028, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0068, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0041, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0112, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0115, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0146, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0100, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0043, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0133, training accuracy 0.7500\n",
      "epoch[0/30] training loss 0.0153, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0041, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0063, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0138, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0090, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0154, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0151, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0067, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0078, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0055, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0045, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0083, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0088, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0100, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0066, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0105, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0094, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0042, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0076, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0171, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0125, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0122, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0038, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0057, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0037, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0175, training accuracy 0.7812\n",
      "epoch[0/30] training loss 0.0070, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0126, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0073, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0079, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0071, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0144, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0054, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0099, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0117, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0074, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0052, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0174, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0065, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0041, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0049, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0131, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0137, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0050, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0101, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0149, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0060, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0050, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0119, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0056, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0111, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0189, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0032, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0031, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0035, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0028, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0044, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0154, training accuracy 0.8125\n",
      "epoch[0/30] training loss 0.0106, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0115, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0052, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0023, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0029, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0033, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0060, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0094, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0038, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0090, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0080, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0040, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0018, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0105, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0078, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0082, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0061, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0043, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0092, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0018, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0036, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0088, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0093, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0076, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0037, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0057, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0024, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0058, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0082, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0031, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0071, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0021, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0053, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0045, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0089, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0074, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0057, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0030, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0052, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0023, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0073, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0049, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0087, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0032, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0062, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0044, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0033, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0105, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0049, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0020, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0019, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0112, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0077, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0126, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0089, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0041, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0054, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0096, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0124, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0057, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0064, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0044, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0072, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0051, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0043, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0042, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0092, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0018, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0038, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0066, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0030, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0064, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0062, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0093, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0044, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0128, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0065, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0020, training accuracy 1.0000\n",
      "epoch[0/30] training loss 0.0069, training accuracy 0.8750\n",
      "epoch[0/30] training loss 0.0063, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0023, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0038, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0064, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0063, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0040, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0110, training accuracy 0.8438\n",
      "epoch[0/30] training loss 0.0049, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0115, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0039, training accuracy 0.9688\n",
      "epoch[0/30] training loss 0.0067, training accuracy 0.9062\n",
      "epoch[0/30] training loss 0.0044, training accuracy 0.9375\n",
      "epoch[0/30] training loss 0.0014, training accuracy 0.5000\n",
      "[val] acc : 0.9278, loss : 0.2144\n",
      "best acc : 0.9278, best loss : 0.2144\n",
      "epoch[1/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0035, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0025, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0038, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0054, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0031, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0030, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0054, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0106, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0104, training accuracy 0.8750\n",
      "epoch[1/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0115, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0023, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0111, training accuracy 0.8750\n",
      "epoch[1/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0087, training accuracy 0.8750\n",
      "epoch[1/30] training loss 0.0049, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0032, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0057, training accuracy 0.8750\n",
      "epoch[1/30] training loss 0.0045, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0026, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0043, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0106, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0048, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0031, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0062, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0050, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0026, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0016, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0028, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0052, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0018, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0048, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0043, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0048, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0046, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0072, training accuracy 0.8750\n",
      "epoch[1/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0039, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0050, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0056, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0018, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0076, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0096, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0018, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0094, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0041, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0057, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0037, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0016, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0056, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0038, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0039, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0069, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0077, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0022, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0025, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0060, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0018, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0023, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0020, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0043, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0048, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0052, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0048, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0032, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0043, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0053, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0017, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0042, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0034, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0047, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0044, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0082, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0061, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0059, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0019, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0026, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0050, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0141, training accuracy 0.8750\n",
      "epoch[1/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0031, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0024, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0051, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0063, training accuracy 0.8438\n",
      "epoch[1/30] training loss 0.0047, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0030, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0040, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0060, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0055, training accuracy 0.8438\n",
      "epoch[1/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0070, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0053, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0025, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0065, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0062, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0034, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0018, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0037, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0045, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0042, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0021, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0019, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0040, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0020, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0071, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0036, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0038, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0039, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0080, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0085, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0089, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0081, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0069, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0026, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0036, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0046, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0059, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0047, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0152, training accuracy 0.8438\n",
      "epoch[1/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0052, training accuracy 0.8750\n",
      "epoch[1/30] training loss 0.0036, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0049, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0036, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0038, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0021, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0063, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0098, training accuracy 0.8750\n",
      "epoch[1/30] training loss 0.0061, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0016, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0043, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0064, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0087, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0025, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0019, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0051, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0053, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0068, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0049, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0087, training accuracy 0.8750\n",
      "epoch[1/30] training loss 0.0048, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0069, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0065, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0070, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0035, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0069, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0055, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0016, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0056, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0025, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0044, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0033, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0031, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0055, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0050, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0045, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0048, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0045, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0040, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0055, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0019, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0041, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0055, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0076, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0025, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0017, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0057, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0016, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0058, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0067, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0025, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0074, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0050, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0056, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0056, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0045, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0069, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0064, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0043, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0028, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0018, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0026, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0056, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0037, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0046, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0019, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0019, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0017, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0050, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0064, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0037, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0060, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0035, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0045, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0029, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0060, training accuracy 0.9062\n",
      "epoch[1/30] training loss 0.0034, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0028, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0073, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0041, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0034, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0016, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0035, training accuracy 0.9375\n",
      "epoch[1/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0018, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0037, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0057, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0054, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[1/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0022, training accuracy 1.0000\n",
      "epoch[1/30] training loss 0.0022, training accuracy 0.4688\n",
      "[val] acc : 0.9540, loss : 0.1362\n",
      "best acc : 0.9540, best loss : 0.1362\n",
      "epoch[2/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0039, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0065, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0037, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0022, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0070, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0050, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0070, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0115, training accuracy 0.9062\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0042, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0048, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0038, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0016, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0036, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0049, training accuracy 0.9062\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0036, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0023, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0038, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0036, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0050, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0041, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0022, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0017, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0042, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0054, training accuracy 0.9062\n",
      "epoch[2/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0037, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0020, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0025, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0037, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0063, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0068, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0036, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0104, training accuracy 0.8750\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0024, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0034, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0031, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0040, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0040, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0119, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0037, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0046, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0064, training accuracy 0.9062\n",
      "epoch[2/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0043, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0045, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0024, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0051, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0049, training accuracy 0.9062\n",
      "epoch[2/30] training loss 0.0033, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0055, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0044, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0053, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0021, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0032, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0050, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0040, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0038, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0045, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0067, training accuracy 0.9062\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0089, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0077, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0039, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0036, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0079, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0038, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0061, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0056, training accuracy 0.9062\n",
      "epoch[2/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0016, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0053, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0027, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0045, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0038, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0029, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0034, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0042, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0029, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0052, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0056, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0066, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0068, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0059, training accuracy 0.9062\n",
      "epoch[2/30] training loss 0.0063, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0042, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0039, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0030, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0031, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0039, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0053, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0039, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0095, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0044, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0045, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0037, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0057, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0050, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0054, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0043, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0050, training accuracy 0.9062\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0041, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0042, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0018, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0049, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0047, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0037, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0066, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0065, training accuracy 0.9688\n",
      "epoch[2/30] training loss 0.0045, training accuracy 0.9062\n",
      "epoch[2/30] training loss 0.0092, training accuracy 0.9062\n",
      "epoch[2/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0058, training accuracy 0.9375\n",
      "epoch[2/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[2/30] training loss 0.0044, training accuracy 0.4688\n",
      "[val] acc : 0.9519, loss : 0.1438\n",
      "best acc : 0.9540, best loss : 0.1362\n",
      "epoch[3/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0041, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0027, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0057, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0082, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0044, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0044, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0044, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0008, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0048, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0046, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0080, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0048, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0031, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0112, training accuracy 0.9062\n",
      "epoch[3/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0053, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0017, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0073, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0036, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0016, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0068, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0036, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0055, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0042, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0017, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0067, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0058, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0032, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0021, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0078, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0081, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0046, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0046, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0030, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0035, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0092, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0029, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0039, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0045, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0087, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0051, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0043, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0034, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0064, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0027, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0043, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0038, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[3/30] training loss 0.0025, training accuracy 0.9375\n",
      "epoch[3/30] training loss 0.0004, training accuracy 0.5000\n",
      "[val] acc : 0.9767, loss : 0.0637\n",
      "best acc : 0.9767, best loss : 0.0637\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0024, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0043, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0032, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0021, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0035, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0059, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0019, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0045, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0042, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0068, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0038, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0059, training accuracy 0.9062\n",
      "epoch[4/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0016, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0047, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0086, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0045, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0020, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0051, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0036, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0034, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0018, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0068, training accuracy 0.8750\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0044, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0046, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0156, training accuracy 0.8438\n",
      "epoch[4/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0046, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0047, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0030, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0063, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0062, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0043, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0034, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0089, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0048, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0027, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0039, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0039, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0016, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0035, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0008, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0088, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0046, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0038, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0036, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0039, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0039, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0064, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0131, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0056, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[4/30] training loss 0.0039, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0115, training accuracy 0.9375\n",
      "epoch[4/30] training loss 0.0036, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[4/30] training loss 0.0029, training accuracy 0.4688\n",
      "[val] acc : 0.9677, loss : 0.1075\n",
      "best acc : 0.9767, best loss : 0.0637\n",
      "epoch[5/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0027, training accuracy 0.9375\n",
      "epoch[5/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0059, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0016, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0008, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0008, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0041, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0050, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0041, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0036, training accuracy 0.9375\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0036, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0008, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0087, training accuracy 0.9062\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0080, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0043, training accuracy 0.9688\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0023, training accuracy 0.9375\n",
      "epoch[5/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[5/30] training loss 0.0001, training accuracy 0.5000\n",
      "[val] acc : 0.9696, loss : 0.0952\n",
      "best acc : 0.9767, best loss : 0.0637\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0053, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0053, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0008, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0063, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0098, training accuracy 0.9062\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0097, training accuracy 0.9062\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0074, training accuracy 0.8750\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0061, training accuracy 0.9062\n",
      "epoch[6/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0064, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0057, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0086, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0055, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0121, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0101, training accuracy 0.9062\n",
      "epoch[6/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0035, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0026, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0046, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0026, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0075, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0059, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0068, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0022, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0092, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0079, training accuracy 0.8750\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0045, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0042, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0023, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0040, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0016, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0032, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0113, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0047, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0069, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0083, training accuracy 0.9062\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0016, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0040, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0050, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0064, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0053, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0072, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0119, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0027, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0088, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0038, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0038, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0017, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0146, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0031, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0023, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0067, training accuracy 0.8750\n",
      "epoch[6/30] training loss 0.0052, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0076, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0046, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0035, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0056, training accuracy 0.9375\n",
      "epoch[6/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[6/30] training loss 0.0003, training accuracy 0.5000\n",
      "[val] acc : 0.9659, loss : 0.1064\n",
      "best acc : 0.9767, best loss : 0.0637\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0042, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0062, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0031, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0043, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0058, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0062, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0020, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0008, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0041, training accuracy 0.9062\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0037, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0040, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0018, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0054, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0048, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0062, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0029, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0061, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0072, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0028, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0046, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0047, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0040, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0107, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0033, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0008, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0058, training accuracy 0.9062\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0020, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0067, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0031, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0037, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0046, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0024, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0105, training accuracy 0.9062\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0033, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0052, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0030, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0047, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0019, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0037, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0029, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0041, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0022, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0093, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0034, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0034, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0017, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0059, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0024, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0022, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0057, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0044, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0040, training accuracy 0.9375\n",
      "epoch[7/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[7/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[7/30] training loss 0.0002, training accuracy 0.5000\n",
      "[val] acc : 0.9680, loss : 0.1028\n",
      "best acc : 0.9767, best loss : 0.0637\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0037, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0099, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0008, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0022, training accuracy 0.9375\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0080, training accuracy 0.9375\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0014, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0036, training accuracy 0.9375\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0045, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0043, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0018, training accuracy 0.9375\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0043, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0029, training accuracy 0.9375\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0040, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0049, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0019, training accuracy 0.9375\n",
      "epoch[8/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0008, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0026, training accuracy 0.9375\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0069, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0054, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0007, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0041, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0155, training accuracy 0.9375\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0043, training accuracy 0.9375\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0029, training accuracy 0.9375\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0049, training accuracy 0.9688\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[8/30] training loss 0.0002, training accuracy 0.5000\n",
      "[val] acc : 0.9878, loss : 0.0386\n",
      "best acc : 0.9878, best loss : 0.0386\n",
      "epoch[9/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0062, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0038, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0069, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0054, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0060, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0100, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0086, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0092, training accuracy 0.9062\n",
      "epoch[9/30] training loss 0.0066, training accuracy 0.9062\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0042, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0029, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0041, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0028, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0066, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0041, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0049, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0015, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0068, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0061, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0032, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0036, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0019, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0019, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0035, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0058, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0048, training accuracy 0.9062\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0073, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0048, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0046, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0037, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0041, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0036, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0065, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0064, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0023, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0012, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0044, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0030, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0007, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0029, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0036, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0054, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0033, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0059, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0035, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0037, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0062, training accuracy 0.9062\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0040, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0048, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0032, training accuracy 0.9375\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0010, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0040, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0021, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[9/30] training loss 0.0001, training accuracy 0.5000\n",
      "[val] acc : 0.9852, loss : 0.0466\n",
      "best acc : 0.9878, best loss : 0.0386\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0015, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0019, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0024, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0032, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0026, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0013, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0045, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0022, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0009, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0008, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0012, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0049, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0045, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0013, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0031, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0010, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0014, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0008, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0008, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0008, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0011, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0016, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0008, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0017, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0007, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0028, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0020, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0027, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0011, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0009, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0025, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0030, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0018, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0005, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0006, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0008, training accuracy 0.9688\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[10/30] training loss 0.0000, training accuracy 0.5000\n",
      "[val] acc : 0.9876, loss : 0.0377\n",
      "best acc : 0.9878, best loss : 0.0377\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0003, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0002, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0004, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0001, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n",
      "epoch[11/30] training loss 0.0000, training accuracy 1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ffab3bfd608b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    basemodel_resnet101.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    for train_batch in train_dataloader_mask:\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outs = basemodel_resnet101(inputs)\n",
    "        preds = torch.argmax(outs, dim=-1)\n",
    "        loss = criterion(outs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        \n",
    "        train_loss = loss_value / batch_size\n",
    "        train_acc = matches / batch_size\n",
    "        print(f\"epoch[{epoch}/{NUM_EPOCH}] training loss {train_loss:.4f}, training accuracy {train_acc:.4f}\")\n",
    "        \n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        basemodel_resnet101.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        for val_batch in val_dataloader_mask:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outs = basemodel_resnet101(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            \n",
    "            loss_item = criterion(outs, labels).item()\n",
    "            acc_item = (labels==preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "            \n",
    "        val_loss = np.sum(val_loss_items) / len(val_dataloader_mask)\n",
    "        val_acc = np.sum(val_acc_items) / len(mask_val_set)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "        print(f\"[val] acc : {val_acc:.4f}, loss : {val_loss:.4f}\")\n",
    "        print(f\"best acc : {best_val_acc:.4f}, best loss : {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afda23d0-6ad9-408b-99c9-705ae5fd8347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbc5c6e168e63498590db46022617123f1fe1268.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0e72482bf56b3581c081f7da2a6180b8792c7089.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b549040c49190cedc41327748aeb197c1670f14d.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ImageID  ans\n",
       "0  cbc5c6e168e63498590db46022617123f1fe1268.jpg    0\n",
       "1  0e72482bf56b3581c081f7da2a6180b8792c7089.jpg    0\n",
       "2  b549040c49190cedc41327748aeb197c1670f14d.jpg    0\n",
       "3  4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg    0\n",
       "4  248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg    0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "test_dir_path = '/opt/ml/input/data/eval/'\n",
    "test_image_path = '/opt/ml/input/data/eval/images/'\n",
    "\n",
    "submission = pd.read_csv(test_dir_path+'info.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "566c2df1-a083-4e27-a655-f2c73d15f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [os.path.join(test_image_path, img_id) for img_id in submission.ImageID]\n",
    "test_image = pd.Series(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9c14322-45a8-40a5-aded-f335204484b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Dataset(Dataset):\n",
    "    def __init__(self, midcrop=True, transform=None):\n",
    "        self.midcrop = midcrop\n",
    "        self.data = test_image\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(test_image)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.midcrop:\n",
    "            img = img[64:448]\n",
    "            \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "138db932-cc09-4f85-89d8-e7485659f7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "dataset = Test_Dataset(transform = transforms.Compose([\n",
    "                            transforms.ToTensor()\n",
    "                        ]))\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = basemodel_resnet101.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir_path, 'submission_resnet101.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd05b0a-17a2-47b4-b10d-de6e4344e098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
