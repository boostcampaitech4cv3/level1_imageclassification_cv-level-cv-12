{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c84b6b-e562-4293-b364-adaf3cc651ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2358d4-dbaf-4af8-a611-3f6a2e35dae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  3% |  0% |\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1d65a3-78de-4b01-a862-e6bd4c7ef135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os\n",
    "from torchmetrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d15beaba-39cc-48da-8165-9f2abc16322a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/ml/input/data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../input/data')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3beb99-f9c7-4026-989a-cbaa8de1f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find . -regex \".*\\.\\_[a-zA-Z0-9._]+\" -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8d19de-5a8e-454f-a22b-129e94792395",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 12\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f69f93c-5675-433f-8a88-7bf8e411d51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>006954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>006955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>006956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>006957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path\n",
       "0     000001  female  Asian   45  000001_female_Asian_45\n",
       "1     000002  female  Asian   52  000002_female_Asian_52\n",
       "2     000004    male  Asian   54    000004_male_Asian_54\n",
       "3     000005  female  Asian   58  000005_female_Asian_58\n",
       "4     000006  female  Asian   59  000006_female_Asian_59\n",
       "...      ...     ...    ...  ...                     ...\n",
       "2695  006954    male  Asian   19    006954_male_Asian_19\n",
       "2696  006955    male  Asian   19    006955_male_Asian_19\n",
       "2697  006956    male  Asian   19    006956_male_Asian_19\n",
       "2698  006957    male  Asian   20    006957_male_Asian_20\n",
       "2699  006959    male  Asian   19    006959_male_Asian_19\n",
       "\n",
       "[2700 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir_path = '/opt/ml/input/data/train/'\n",
    "train_image_path = '/opt/ml/input/data/train/images/'\n",
    "\n",
    "dt_train = pd.read_csv(train_dir_path+'train.csv')\n",
    "dt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3377e12b-fd28-4837-a749-bba0d75ff944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age_range(age):\n",
    "    if age < 30:\n",
    "        return 0\n",
    "    elif 30 <= age < 60:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9757722a-75bc-42f9-a3ff-3fca7d10cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train['age_range'] = dt_train['age'].apply(lambda x : get_age_range(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a95f3599-8a5f-4815-91d5-673118deaced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "      <th>age_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>006954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>006955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>006956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>006957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path  age_range\n",
       "0     000001  female  Asian   45  000001_female_Asian_45          1\n",
       "1     000002  female  Asian   52  000002_female_Asian_52          1\n",
       "2     000004    male  Asian   54    000004_male_Asian_54          1\n",
       "3     000005  female  Asian   58  000005_female_Asian_58          1\n",
       "4     000006  female  Asian   59  000006_female_Asian_59          1\n",
       "...      ...     ...    ...  ...                     ...        ...\n",
       "2695  006954    male  Asian   19    006954_male_Asian_19          0\n",
       "2696  006955    male  Asian   19    006955_male_Asian_19          0\n",
       "2697  006956    male  Asian   19    006956_male_Asian_19          0\n",
       "2698  006957    male  Asian   20    006957_male_Asian_20          0\n",
       "2699  006959    male  Asian   19    006959_male_Asian_19          0\n",
       "\n",
       "[2700 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c164652-b2c8-4340-b55d-6ba9108ad9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, valid_idx = train_test_split(np.arange(len(dt_train)),\n",
    "                                       test_size=0.2,\n",
    "                                       shuffle=True,\n",
    "                                       stratify=dt_train['age_range'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c53d195f-6e0c-4af7-a558-30ad3481e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = []\n",
    "train_label = []\n",
    "\n",
    "for idx in train_idx:\n",
    "    path = dt_train.iloc[idx]['path']\n",
    "    for file_name in [i for i in os.listdir(train_image_path+path) if i[0] != '.']:\n",
    "        train_image.append(train_image_path+path+'/'+file_name)\n",
    "        train_label.append((path.split('_')[1], path.split('_')[3], file_name.split('.')[0]))                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d5b59fc-3b23-4b0a-a747-a3197958daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_image = []\n",
    "valid_label = []\n",
    "\n",
    "for idx in valid_idx:\n",
    "    path = dt_train.iloc[idx]['path']\n",
    "    for file_name in [i for i in os.listdir(train_image_path+path) if i[0] != '.']:\n",
    "        valid_image.append(train_image_path+path+'/'+file_name)\n",
    "        valid_label.append((path.split('_')[1], path.split('_')[3], file_name.split('.')[0]))                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98de1eb4-55d4-41ba-8541-70fff9bb011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_enc(x):\n",
    "    def gender(i):\n",
    "        if i == 'male':\n",
    "            return 0\n",
    "        elif i == 'female':\n",
    "            return 3\n",
    "    def age(j):\n",
    "        j = int(j)\n",
    "        if j < 30:\n",
    "            return 0\n",
    "        elif j >= 30 and j < 60:\n",
    "            return 1\n",
    "        elif j >= 60:\n",
    "            return 2\n",
    "    def mask(k):\n",
    "        if k == 'normal':\n",
    "            return 12\n",
    "        elif 'incorrect' in k:\n",
    "            return 6\n",
    "        else:\n",
    "            return 0\n",
    "    return gender(x[0]) + age(x[1]) + mask(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "694474c7-7175-40e6-b071-e50218629f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sr_data = pd.Series(whole_image_path)\n",
    "# sr_label = pd.Series(whole_target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "943d0388-3736-4b6b-a624-0213148616f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.Series(train_image)\n",
    "train_label = pd.Series(train_label)\n",
    "\n",
    "valid_data = pd.Series(valid_image)\n",
    "valid_label = pd.Series(valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2cd7b29-5f03-444e-b781-902ca74f2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Mask(Dataset):\n",
    "    def __init__(self, data, label, encoding=True, midcrop=True, transform=None):\n",
    "        self.encoding = encoding\n",
    "        self.midcrop = midcrop\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        \n",
    "        if encoding:\n",
    "            self.label = self.label.apply(onehot_enc)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        y = self.label[idx]\n",
    "        \n",
    "        if self.midcrop:\n",
    "            X = X[80:380, 42:342]\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(X), y\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2a22aff-e634-4f15-877c-752d450d01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train_set = Dataset_Mask(data=train_data, label=train_label, transform = transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e78516b2-b620-495c-bd6f-81baa034e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_val_set = Dataset_Mask(data=valid_data, label=valid_label, transform = transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0989b05b-41b0-4a8f-9987-f062b7ad2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_image = [mask_train_set[i][1] for i in range(len(mask_train_set))]\n",
    "v_image = [mask_val_set[i][1] for i in range(len(mask_val_set))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09085d73-8a39-434b-bab3-9da358c5dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = pd.DataFrame(t_image, columns=['counts'])\n",
    "v_df = pd.DataFrame(v_image, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7da846b8-9898-43ce-8a29-dc44483a71e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'valid set labels')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.countplot(x='counts', data=t_df, ax=axes[0])\n",
    "axes[0].set_xlabel(\"train set labels\")\n",
    "sns.countplot(x='counts', data=v_df, ax=axes[1])\n",
    "axes[1].set_xlabel(\"valid set labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff343e87-8482-4c01-abca-593a1d433226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size : 15120\n",
      "validation data size : 3780\n"
     ]
    }
   ],
   "source": [
    "print(f'training data size : {len(mask_train_set)}')\n",
    "print(f'validation data size : {len(mask_val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0612969-f203-4b93-8607-9b6f291be475",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_dataloader_mask = DataLoader(dataset = mask_train_set, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader_mask = DataLoader(dataset = mask_val_set, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af71a4ef-a137-47b0-af94-eebedaa36014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필요 입력 채널 개수 3\n",
      "네트워크 출력 채널 개수 1000\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "basemodel_resnet34 = torchvision.models.resnet34(pretrained=True)\n",
    "print('필요 입력 채널 개수', basemodel_resnet34.conv1.weight.shape[1])\n",
    "print('네트워크 출력 채널 개수', basemodel_resnet34.fc.weight.shape[0])\n",
    "print(basemodel_resnet34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11357fc4-548b-452e-bbd8-e60ef6261308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필요 입력 채널 개수 3\n",
      "네트워크 출력 채널 개수 18\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "class_num = 18\n",
    "basemodel_resnet34.fc = nn.Linear(in_features=512, out_features=class_num, bias=True)\n",
    "nn.init.xavier_uniform_(basemodel_resnet34.fc.weight)\n",
    "stdv = 1. / math.sqrt(basemodel_resnet34.fc.weight.size(1))\n",
    "basemodel_resnet34.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "print('필요 입력 채널 개수', basemodel_resnet34.conv1.weight.shape[1])\n",
    "print('네트워크 출력 채널 개수', basemodel_resnet34.fc.weight.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a1a2262-f820-4294-8086-3a48a5e5d686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")\n",
    "\n",
    "basemodel_resnet34.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCH = 50\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(basemodel_resnet34.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64a36f92-aad5-4bbb-a920-29d50f0f058b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] name:[conv1.weight] shape:[(64, 3, 7, 7)].\n",
      "    val:[ 0.005 -0.007  0.008  0.038  0.049]\n",
      "[1] name:[bn1.weight] shape:[(64,)].\n",
      "    val:[0.302 0.268 0.26  0.311 0.238]\n",
      "[2] name:[bn1.bias] shape:[(64,)].\n",
      "    val:[0.481 0.207 0.331 0.38  0.094]\n",
      "[3] name:[layer1.0.conv1.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[-0.005  0.015 -0.006 -0.06  -0.024]\n",
      "[4] name:[layer1.0.bn1.weight] shape:[(64,)].\n",
      "    val:[0.24  0.185 0.216 0.165 0.181]\n",
      "[5] name:[layer1.0.bn1.bias] shape:[(64,)].\n",
      "    val:[0.025 0.088 0.082 0.142 0.066]\n",
      "[6] name:[layer1.0.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 0.066 -0.01   0.041  0.033 -0.055]\n",
      "[7] name:[layer1.0.bn2.weight] shape:[(64,)].\n",
      "    val:[0.34  0.187 0.252 0.307 0.259]\n",
      "[8] name:[layer1.0.bn2.bias] shape:[(64,)].\n",
      "    val:[-0.251  0.196  0.23  -0.114  0.07 ]\n",
      "[9] name:[layer1.1.conv1.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[-0.008 -0.04  -0.054 -0.019  0.011]\n",
      "[10] name:[layer1.1.bn1.weight] shape:[(64,)].\n",
      "    val:[0.178 0.373 0.18  0.26  0.246]\n",
      "[11] name:[layer1.1.bn1.bias] shape:[(64,)].\n",
      "    val:[ 0.073 -0.222  0.177 -0.063 -0.051]\n",
      "[12] name:[layer1.1.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[-0.002 -0.04  -0.012  0.008  0.088]\n",
      "[13] name:[layer1.1.bn2.weight] shape:[(64,)].\n",
      "    val:[0.417 0.209 0.224 0.179 0.402]\n",
      "[14] name:[layer1.1.bn2.bias] shape:[(64,)].\n",
      "    val:[-0.033 -0.08   0.174 -0.102  0.176]\n",
      "[15] name:[layer1.2.conv1.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 0.021 -0.101 -0.024  0.013  0.111]\n",
      "[16] name:[layer1.2.bn1.weight] shape:[(64,)].\n",
      "    val:[0.335 0.203 0.192 0.247 0.248]\n",
      "[17] name:[layer1.2.bn1.bias] shape:[(64,)].\n",
      "    val:[-0.266 -0.041 -0.11  -0.246  0.017]\n",
      "[18] name:[layer1.2.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[-0.017  0.023 -0.021 -0.04   0.042]\n",
      "[19] name:[layer1.2.bn2.weight] shape:[(64,)].\n",
      "    val:[0.554 0.169 0.286 0.193 0.248]\n",
      "[20] name:[layer1.2.bn2.bias] shape:[(64,)].\n",
      "    val:[-0.093 -0.217  0.121  0.058  0.072]\n",
      "[21] name:[layer2.0.conv1.weight] shape:[(128, 64, 3, 3)].\n",
      "    val:[-0.007  0.001 -0.007 -0.024  0.034]\n",
      "[22] name:[layer2.0.bn1.weight] shape:[(128,)].\n",
      "    val:[0.261 0.288 0.174 0.238 0.274]\n",
      "[23] name:[layer2.0.bn1.bias] shape:[(128,)].\n",
      "    val:[-0.083 -0.094  0.286 -0.043 -0.101]\n",
      "[24] name:[layer2.0.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.005 -0.025  0.025  0.007 -0.021]\n",
      "[25] name:[layer2.0.bn2.weight] shape:[(128,)].\n",
      "    val:[0.376 0.01  0.186 0.269 0.344]\n",
      "[26] name:[layer2.0.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.068  0.218  0.066  0.078  0.135]\n",
      "[27] name:[layer2.0.downsample.0.weight] shape:[(128, 64, 1, 1)].\n",
      "    val:[-0.008 -0.109  0.045 -0.033 -0.003]\n",
      "[28] name:[layer2.0.downsample.1.weight] shape:[(128,)].\n",
      "    val:[0.169 0.36  0.406 0.076 0.207]\n",
      "[29] name:[layer2.0.downsample.1.bias] shape:[(128,)].\n",
      "    val:[-0.068  0.218  0.066  0.078  0.135]\n",
      "[30] name:[layer2.1.conv1.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.011 -0.004  0.012 -0.015 -0.011]\n",
      "[31] name:[layer2.1.bn1.weight] shape:[(128,)].\n",
      "    val:[0.157 0.302 0.154 0.311 0.211]\n",
      "[32] name:[layer2.1.bn1.bias] shape:[(128,)].\n",
      "    val:[ 0.063 -0.199  0.054 -0.259 -0.036]\n",
      "[33] name:[layer2.1.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[ 0.022 -0.006  0.029  0.001 -0.023]\n",
      "[34] name:[layer2.1.bn2.weight] shape:[(128,)].\n",
      "    val:[0.146 0.271 0.169 0.162 0.091]\n",
      "[35] name:[layer2.1.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.064 -0.481 -0.127 -0.03  -0.03 ]\n",
      "[36] name:[layer2.2.conv1.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[ 0.039  0.004 -0.007  0.008  0.01 ]\n",
      "[37] name:[layer2.2.bn1.weight] shape:[(128,)].\n",
      "    val:[0.28  0.303 0.248 0.216 0.235]\n",
      "[38] name:[layer2.2.bn1.bias] shape:[(128,)].\n",
      "    val:[-0.211 -0.532 -0.127 -0.146 -0.088]\n",
      "[39] name:[layer2.2.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.04  -0.046 -0.016 -0.017  0.006]\n",
      "[40] name:[layer2.2.bn2.weight] shape:[(128,)].\n",
      "    val:[0.25  0.102 0.138 0.317 0.144]\n",
      "[41] name:[layer2.2.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.051  0.006  0.007 -0.147  0.035]\n",
      "[42] name:[layer2.3.conv1.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.016 -0.033  0.011 -0.001 -0.038]\n",
      "[43] name:[layer2.3.bn1.weight] shape:[(128,)].\n",
      "    val:[0.216 0.181 0.23  0.205 0.245]\n",
      "[44] name:[layer2.3.bn1.bias] shape:[(128,)].\n",
      "    val:[-0.179 -0.142 -0.17  -0.189 -0.196]\n",
      "[45] name:[layer2.3.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[ 0.012 -0.026 -0.027 -0.002 -0.029]\n",
      "[46] name:[layer2.3.bn2.weight] shape:[(128,)].\n",
      "    val:[ 0.194 -0.018  0.164  0.281  0.151]\n",
      "[47] name:[layer2.3.bn2.bias] shape:[(128,)].\n",
      "    val:[ 0.117  0.032 -0.26  -0.127 -0.086]\n",
      "[48] name:[layer3.0.conv1.weight] shape:[(256, 128, 3, 3)].\n",
      "    val:[ 0.007 -0.013  0.008 -0.018 -0.007]\n",
      "[49] name:[layer3.0.bn1.weight] shape:[(256,)].\n",
      "    val:[0.274 0.267 0.268 0.232 0.234]\n",
      "[50] name:[layer3.0.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.06  -0.081 -0.092  0.066  0.037]\n",
      "[51] name:[layer3.0.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.007  0.005 -0.012 -0.008 -0.003]\n",
      "[52] name:[layer3.0.bn2.weight] shape:[(256,)].\n",
      "    val:[0.349 0.329 0.257 0.217 0.355]\n",
      "[53] name:[layer3.0.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.097 -0.09   0.072  0.154 -0.044]\n",
      "[54] name:[layer3.0.downsample.0.weight] shape:[(256, 128, 1, 1)].\n",
      "    val:[-0.022  0.023  0.002 -0.014 -0.019]\n",
      "[55] name:[layer3.0.downsample.1.weight] shape:[(256,)].\n",
      "    val:[0.143 0.159 0.071 0.082 0.114]\n",
      "[56] name:[layer3.0.downsample.1.bias] shape:[(256,)].\n",
      "    val:[-0.097 -0.09   0.072  0.154 -0.044]\n",
      "[57] name:[layer3.1.conv1.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.001 -0.01  -0.013  0.001 -0.002]\n",
      "[58] name:[layer3.1.bn1.weight] shape:[(256,)].\n",
      "    val:[0.22  0.208 0.202 0.218 0.215]\n",
      "[59] name:[layer3.1.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.185 -0.167 -0.168 -0.18  -0.068]\n",
      "[60] name:[layer3.1.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.004  0.004  0.016 -0.006 -0.01 ]\n",
      "[61] name:[layer3.1.bn2.weight] shape:[(256,)].\n",
      "    val:[0.25  0.2   0.122 0.093 0.191]\n",
      "[62] name:[layer3.1.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.137 -0.085 -0.053 -0.143 -0.108]\n",
      "[63] name:[layer3.2.conv1.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.039 -0.005  0.003  0.02   0.02 ]\n",
      "[64] name:[layer3.2.bn1.weight] shape:[(256,)].\n",
      "    val:[0.234 0.232 0.264 0.187 0.217]\n",
      "[65] name:[layer3.2.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.25  -0.252 -0.323 -0.165 -0.182]\n",
      "[66] name:[layer3.2.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.009 -0.025  0.01   0.021 -0.024]\n",
      "[67] name:[layer3.2.bn2.weight] shape:[(256,)].\n",
      "    val:[0.304 0.184 0.146 0.06  0.261]\n",
      "[68] name:[layer3.2.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.163 -0.158 -0.014 -0.    -0.137]\n",
      "[69] name:[layer3.3.conv1.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.038 -0.003  0.034 -0.018 -0.012]\n",
      "[70] name:[layer3.3.bn1.weight] shape:[(256,)].\n",
      "    val:[0.212 0.206 0.159 0.265 0.208]\n",
      "[71] name:[layer3.3.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.225 -0.162 -0.17  -0.415 -0.243]\n",
      "[72] name:[layer3.3.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.01  -0.014  0.006 -0.005  0.017]\n",
      "[73] name:[layer3.3.bn2.weight] shape:[(256,)].\n",
      "    val:[0.399 0.206 0.196 0.065 0.27 ]\n",
      "[74] name:[layer3.3.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.316 -0.16  -0.087  0.015 -0.196]\n",
      "[75] name:[layer3.4.conv1.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.002 -0.023 -0.029  0.015 -0.   ]\n",
      "[76] name:[layer3.4.bn1.weight] shape:[(256,)].\n",
      "    val:[0.196 0.196 0.241 0.226 0.21 ]\n",
      "[77] name:[layer3.4.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.207 -0.197 -0.268 -0.29  -0.261]\n",
      "[78] name:[layer3.4.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.005  0.001 -0.005  0.028  0.014]\n",
      "[79] name:[layer3.4.bn2.weight] shape:[(256,)].\n",
      "    val:[0.322 0.201 0.109 0.01  0.282]\n",
      "[80] name:[layer3.4.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.196 -0.123 -0.071 -0.001 -0.24 ]\n",
      "[81] name:[layer3.5.conv1.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.016  0.005 -0.004  0.028  0.03 ]\n",
      "[82] name:[layer3.5.bn1.weight] shape:[(256,)].\n",
      "    val:[0.287 0.194 0.268 0.239 0.229]\n",
      "[83] name:[layer3.5.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.274 -0.299 -0.355 -0.27  -0.238]\n",
      "[84] name:[layer3.5.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.002  0.012  0.006 -0.007 -0.005]\n",
      "[85] name:[layer3.5.bn2.weight] shape:[(256,)].\n",
      "    val:[ 0.358  0.252  0.161 -0.03   0.292]\n",
      "[86] name:[layer3.5.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.317 -0.187  0.042 -0.006 -0.33 ]\n",
      "[87] name:[layer4.0.conv1.weight] shape:[(512, 256, 3, 3)].\n",
      "    val:[0.023 0.048 0.058 0.017 0.017]\n",
      "[88] name:[layer4.0.bn1.weight] shape:[(512,)].\n",
      "    val:[0.275 0.262 0.282 0.279 0.251]\n",
      "[89] name:[layer4.0.bn1.bias] shape:[(512,)].\n",
      "    val:[-0.271 -0.201 -0.171 -0.224 -0.2  ]\n",
      "[90] name:[layer4.0.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[ 0.001  0.001 -0.002 -0.013 -0.018]\n",
      "[91] name:[layer4.0.bn2.weight] shape:[(512,)].\n",
      "    val:[0.7   0.664 0.803 0.695 0.743]\n",
      "[92] name:[layer4.0.bn2.bias] shape:[(512,)].\n",
      "    val:[-0.072 -0.131 -0.151 -0.076 -0.066]\n",
      "[93] name:[layer4.0.downsample.0.weight] shape:[(512, 256, 1, 1)].\n",
      "    val:[-0.004 -0.033  0.005  0.037 -0.014]\n",
      "[94] name:[layer4.0.downsample.1.weight] shape:[(512,)].\n",
      "    val:[0.325 0.38  0.501 0.42  0.451]\n",
      "[95] name:[layer4.0.downsample.1.bias] shape:[(512,)].\n",
      "    val:[-0.072 -0.131 -0.151 -0.076 -0.066]\n",
      "[96] name:[layer4.1.conv1.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[ 0.005 -0.005  0.007  0.003  0.003]\n",
      "[97] name:[layer4.1.bn1.weight] shape:[(512,)].\n",
      "    val:[0.247 0.23  0.21  0.258 0.288]\n",
      "[98] name:[layer4.1.bn1.bias] shape:[(512,)].\n",
      "    val:[-0.269 -0.243 -0.198 -0.25  -0.327]\n",
      "[99] name:[layer4.1.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[0.002 0.009 0.004 0.002 0.008]\n",
      "[100] name:[layer4.1.bn2.weight] shape:[(512,)].\n",
      "    val:[0.592 0.595 0.524 0.572 0.548]\n",
      "[101] name:[layer4.1.bn2.bias] shape:[(512,)].\n",
      "    val:[-0.142 -0.179 -0.186 -0.113 -0.115]\n",
      "[102] name:[layer4.2.conv1.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[0.021 0.028 0.021 0.049 0.059]\n",
      "[103] name:[layer4.2.bn1.weight] shape:[(512,)].\n",
      "    val:[0.26  0.228 0.275 0.229 0.224]\n",
      "[104] name:[layer4.2.bn1.bias] shape:[(512,)].\n",
      "    val:[-0.215 -0.164 -0.275 -0.147 -0.161]\n",
      "[105] name:[layer4.2.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[0.042 0.035 0.04  0.032 0.017]\n",
      "[106] name:[layer4.2.bn2.weight] shape:[(512,)].\n",
      "    val:[1.436 1.512 1.565 1.291 1.287]\n",
      "[107] name:[layer4.2.bn2.bias] shape:[(512,)].\n",
      "    val:[0.122 0.129 0.193 0.133 0.098]\n",
      "[108] name:[fc.weight] shape:[(18, 512)].\n",
      "    val:[-0.054 -0.072  0.001 -0.048 -0.083]\n",
      "[109] name:[fc.bias] shape:[(18,)].\n",
      "    val:[ 0.017 -0.026  0.006 -0.042 -0.039]\n",
      "Total number of parameters:[21,293,906].\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "n_param = 0\n",
    "for p_idx, (param_name, param) in enumerate(basemodel_resnet34.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        param_numpy = param.detach().cpu().numpy()\n",
    "        n_param += len(param_numpy.reshape(-1))\n",
    "        print (\"[%d] name:[%s] shape:[%s].\"%(p_idx,param_name,param_numpy.shape))\n",
    "        print (\"    val:%s\"%(param_numpy.reshape(-1)[:5]))\n",
    "print (\"Total number of parameters:[%s].\"%(format(n_param,',d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c067d0e-38c9-4ab8-baba-88cf8dcb8c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[0/50] training loss 0.005, training accuracy 0.027\n",
      "[val] acc : 0.850, loss : 0.431, f1 score: 0.710\n",
      "best acc : 0.850, best loss : 0.431, best f1 : 0.710\n",
      "epoch[1/50] training loss 0.001, training accuracy 0.062\n",
      "[val] acc : 0.851, loss : 0.461, f1 score: 0.696\n",
      "best acc : 0.851, best loss : 0.431, best f1 : 0.710\n",
      "epoch[2/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.831, loss : 0.587, f1 score: 0.712\n",
      "best acc : 0.851, best loss : 0.431, best f1 : 0.712\n",
      "epoch[3/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.795, loss : 0.834, f1 score: 0.578\n",
      "best acc : 0.851, best loss : 0.431, best f1 : 0.712\n",
      "epoch[4/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.860, loss : 0.658, f1 score: 0.702\n",
      "best acc : 0.860, best loss : 0.431, best f1 : 0.712\n",
      "epoch[5/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.883, loss : 0.510, f1 score: 0.710\n",
      "best acc : 0.883, best loss : 0.431, best f1 : 0.712\n",
      "epoch[6/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.892, loss : 0.526, f1 score: 0.706\n",
      "best acc : 0.892, best loss : 0.431, best f1 : 0.712\n",
      "epoch[7/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.893, loss : 0.542, f1 score: 0.755\n",
      "best acc : 0.893, best loss : 0.431, best f1 : 0.755\n",
      "epoch[8/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.897, loss : 0.506, f1 score: 0.741\n",
      "best acc : 0.897, best loss : 0.431, best f1 : 0.755\n",
      "epoch[9/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.897, loss : 0.509, f1 score: 0.743\n",
      "best acc : 0.897, best loss : 0.431, best f1 : 0.755\n",
      "epoch[10/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.897, loss : 0.515, f1 score: 0.743\n",
      "best acc : 0.897, best loss : 0.431, best f1 : 0.755\n",
      "epoch[11/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.897, loss : 0.520, f1 score: 0.738\n",
      "best acc : 0.897, best loss : 0.431, best f1 : 0.755\n",
      "epoch[12/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.897, loss : 0.524, f1 score: 0.741\n",
      "best acc : 0.897, best loss : 0.431, best f1 : 0.755\n",
      "epoch[13/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.897, loss : 0.529, f1 score: 0.741\n",
      "best acc : 0.897, best loss : 0.431, best f1 : 0.755\n",
      "epoch[14/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.898, loss : 0.533, f1 score: 0.741\n",
      "best acc : 0.898, best loss : 0.431, best f1 : 0.755\n",
      "epoch[15/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.898, loss : 0.537, f1 score: 0.741\n",
      "best acc : 0.898, best loss : 0.431, best f1 : 0.755\n",
      "epoch[16/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.898, loss : 0.541, f1 score: 0.741\n",
      "best acc : 0.898, best loss : 0.431, best f1 : 0.755\n",
      "epoch[17/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.899, loss : 0.545, f1 score: 0.744\n",
      "best acc : 0.899, best loss : 0.431, best f1 : 0.755\n",
      "epoch[18/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.899, loss : 0.548, f1 score: 0.744\n",
      "best acc : 0.899, best loss : 0.431, best f1 : 0.755\n",
      "epoch[19/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.552, f1 score: 0.747\n",
      "best acc : 0.900, best loss : 0.431, best f1 : 0.755\n",
      "epoch[20/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.555, f1 score: 0.747\n",
      "best acc : 0.900, best loss : 0.431, best f1 : 0.755\n",
      "epoch[21/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.559, f1 score: 0.747\n",
      "best acc : 0.900, best loss : 0.431, best f1 : 0.755\n",
      "epoch[22/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.562, f1 score: 0.747\n",
      "best acc : 0.900, best loss : 0.431, best f1 : 0.755\n",
      "epoch[23/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.565, f1 score: 0.747\n",
      "best acc : 0.900, best loss : 0.431, best f1 : 0.755\n",
      "epoch[24/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.569, f1 score: 0.747\n",
      "best acc : 0.900, best loss : 0.431, best f1 : 0.755\n",
      "epoch[25/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.572, f1 score: 0.747\n",
      "best acc : 0.900, best loss : 0.431, best f1 : 0.755\n",
      "epoch[26/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.575, f1 score: 0.747\n",
      "best acc : 0.900, best loss : 0.431, best f1 : 0.755\n",
      "epoch[27/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.901, loss : 0.578, f1 score: 0.760\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.760\n",
      "epoch[28/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.901, loss : 0.581, f1 score: 0.760\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.760\n",
      "epoch[29/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.583, f1 score: 0.762\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.762\n",
      "epoch[30/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.586, f1 score: 0.762\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.762\n",
      "epoch[31/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.589, f1 score: 0.762\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.762\n",
      "epoch[32/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.592, f1 score: 0.762\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.762\n",
      "epoch[33/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.595, f1 score: 0.762\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.762\n",
      "epoch[34/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.597, f1 score: 0.762\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.762\n",
      "epoch[35/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.600, f1 score: 0.762\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.762\n",
      "epoch[36/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.602, f1 score: 0.762\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.762\n",
      "epoch[37/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.605, f1 score: 0.762\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.762\n",
      "epoch[38/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.608, f1 score: 0.766\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.766\n",
      "epoch[39/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.610, f1 score: 0.766\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.766\n",
      "epoch[40/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.613, f1 score: 0.766\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.766\n",
      "epoch[41/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.615, f1 score: 0.766\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.766\n",
      "epoch[42/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.617, f1 score: 0.766\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.766\n",
      "epoch[43/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.620, f1 score: 0.766\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.766\n",
      "epoch[44/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.900, loss : 0.622, f1 score: 0.766\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.766\n",
      "epoch[45/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.901, loss : 0.625, f1 score: 0.766\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.766\n",
      "epoch[46/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.901, loss : 0.627, f1 score: 0.766\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.766\n",
      "epoch[47/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.901, loss : 0.629, f1 score: 0.766\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.766\n",
      "epoch[48/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.901, loss : 0.631, f1 score: 0.766\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.766\n",
      "epoch[49/50] training loss 0.000, training accuracy 0.062\n",
      "[val] acc : 0.901, loss : 0.634, f1 score: 0.766\n",
      "best acc : 0.901, best loss : 0.431, best f1 : 0.766\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "patience = 10\n",
    "cur_count = 0\n",
    "\n",
    "f1 = F1Score(num_classes=class_num, average='macro').to(device)\n",
    "best_f1_score = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    basemodel_resnet34.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    for train_batch in train_dataloader_mask:\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outs = basemodel_resnet34(inputs)\n",
    "        preds = torch.argmax(outs, dim=-1)\n",
    "        loss = criterion(outs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(basemodel_resnet34, '/opt/ml/checkpoint/resnet34/checkpoint_ep_%d.pt'% epoch)\n",
    "        \n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        \n",
    "        train_loss = loss_value / batch_size\n",
    "        train_acc = matches / batch_size\n",
    "        \n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "    print(f\"epoch[{epoch}/{NUM_EPOCH}] training loss {train_loss:.3f}, training accuracy {train_acc:.3f}\")\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        basemodel_resnet34.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        for val_batch in val_dataloader_mask:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outs = basemodel_resnet34(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            \n",
    "            loss_item = criterion(outs, labels).item()\n",
    "            acc_item = (labels==preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "            \n",
    "        val_loss = np.sum(val_loss_items) / len(val_dataloader_mask)\n",
    "        val_acc = np.sum(val_acc_items) / len(mask_val_set)\n",
    "\n",
    "        f1_score = f1(outs, labels)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "        if f1_score > best_f1_score:\n",
    "            best_f1_score = f1_score\n",
    "#             cur_count = 0\n",
    "            torch.save(basemodel_resnet34, '/opt/ml/checkpoint/resnet34/checkpoint_best.pt')\n",
    "#         else:\n",
    "#             cur_count += 1\n",
    "#             if cur_count >= patience:\n",
    "#                 print(\"Early Stopping!\")\n",
    "#                 break\n",
    "            \n",
    "            \n",
    "        print(f\"[val] acc : {val_acc:.3f}, loss : {val_loss:.3f}, f1 score: {f1_score:.3f}\")\n",
    "        print(f\"best acc : {best_val_acc:.3f}, best loss : {best_val_loss:.3f}, best f1 : {best_f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94e6d274-764e-4dca-9297-23241fb569b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1 score:0.7656214237213135\n"
     ]
    }
   ],
   "source": [
    "print(f'Best f1 score:{best_f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afda23d0-6ad9-408b-99c9-705ae5fd8347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbc5c6e168e63498590db46022617123f1fe1268.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0e72482bf56b3581c081f7da2a6180b8792c7089.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b549040c49190cedc41327748aeb197c1670f14d.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ImageID  ans\n",
       "0  cbc5c6e168e63498590db46022617123f1fe1268.jpg    0\n",
       "1  0e72482bf56b3581c081f7da2a6180b8792c7089.jpg    0\n",
       "2  b549040c49190cedc41327748aeb197c1670f14d.jpg    0\n",
       "3  4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg    0\n",
       "4  248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg    0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "test_dir_path = '/opt/ml/input/data/eval/'\n",
    "test_image_path = '/opt/ml/input/data/eval/images/'\n",
    "\n",
    "basemodel_resnet34 = torch.load('/opt/ml/checkpoint/resnet34/checkpoint_best.pt')\n",
    "submission = pd.read_csv(test_dir_path+'info.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "566c2df1-a083-4e27-a655-f2c73d15f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [os.path.join(test_image_path, img_id) for img_id in submission.ImageID]\n",
    "test_image = pd.Series(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9c14322-45a8-40a5-aded-f335204484b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Dataset(Dataset):\n",
    "    def __init__(self, midcrop=True, transform=None):\n",
    "        self.midcrop = midcrop\n",
    "        self.data = test_image\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(test_image)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.midcrop:\n",
    "            img = img[64:448]\n",
    "            \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "138db932-cc09-4f85-89d8-e7485659f7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "dataset = Test_Dataset(transform = transforms.Compose([\n",
    "                            transforms.ToTensor()\n",
    "                        ]))\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = basemodel_resnet34.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir_path, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd05b0a-17a2-47b4-b10d-de6e4344e098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b7d263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
