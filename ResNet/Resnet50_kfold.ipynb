{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c84b6b-e562-4293-b364-adaf3cc651ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2358d4-dbaf-4af8-a611-3f6a2e35dae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  3% |\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1d65a3-78de-4b01-a862-e6bd4c7ef135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "from torchmetrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d15beaba-39cc-48da-8165-9f2abc16322a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/ml/input/data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/opt/ml/input/data')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3beb99-f9c7-4026-989a-cbaa8de1f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find . -regex \".*\\.\\_[a-zA-Z0-9._]+\" -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8d19de-5a8e-454f-a22b-129e94792395",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 12\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f69f93c-5675-433f-8a88-7bf8e411d51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>6954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>6955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>6956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>6957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>6959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  gender   race  age                    path\n",
       "0        1  female  Asian   45  000001_female_Asian_45\n",
       "1        2  female  Asian   52  000002_female_Asian_52\n",
       "2        4    male  Asian   54    000004_male_Asian_54\n",
       "3        5  female  Asian   58  000005_female_Asian_58\n",
       "4        6  female  Asian   59  000006_female_Asian_59\n",
       "...    ...     ...    ...  ...                     ...\n",
       "2695  6954    male  Asian   19    006954_male_Asian_19\n",
       "2696  6955    male  Asian   19    006955_male_Asian_19\n",
       "2697  6956    male  Asian   19    006956_male_Asian_19\n",
       "2698  6957    male  Asian   20    006957_male_Asian_20\n",
       "2699  6959    male  Asian   19    006959_male_Asian_19\n",
       "\n",
       "[2700 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir_path = '/opt/ml/input/data/train/'\n",
    "train_image_path = '/opt/ml/input/data/train/images/'\n",
    "\n",
    "dt_train = pd.read_csv(train_dir_path+'train.csv')\n",
    "dt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3377e12b-fd28-4837-a749-bba0d75ff944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age_range(age):\n",
    "    if age < 30:\n",
    "        return 0\n",
    "    elif 30 <= age < 60:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "962b5a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file, gender, age):\n",
    "    if 'normal'in file:\n",
    "        # Not Wear\n",
    "        if gender =='male':\n",
    "            if age < 30: \n",
    "                return 12\n",
    "            elif age < 60:\n",
    "                return 13\n",
    "            else:\n",
    "                return 14\n",
    "        else: # female\n",
    "            if age < 30:\n",
    "                return 15\n",
    "            elif age < 60:\n",
    "                return 16\n",
    "            else:\n",
    "                return 17\n",
    "    elif 'incorrect' in file:\n",
    "        # Incorrect\n",
    "        if gender == 'male':\n",
    "            if age < 30: \n",
    "                return 6\n",
    "            elif age < 60:\n",
    "                return 7\n",
    "            else:\n",
    "                return 8\n",
    "        else: # female\n",
    "            if age < 30:\n",
    "                return 9\n",
    "            elif age < 60:\n",
    "                return 10\n",
    "            else:\n",
    "                return 11\n",
    "    else:\n",
    "        # Wear\n",
    "        if gender == 'male':\n",
    "            if age < 30: \n",
    "                return 0\n",
    "            elif age < 60:\n",
    "                return 1\n",
    "            else:\n",
    "                return 2\n",
    "        else: # female\n",
    "            if age < 30:\n",
    "                return 3\n",
    "            elif age < 60:\n",
    "                return 4\n",
    "            else:\n",
    "                return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9757722a-75bc-42f9-a3ff-3fca7d10cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train['age_range'] = dt_train['age'].apply(lambda x : get_age_range(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f63f5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train['label'] = -1\n",
    "\n",
    "for index, row in dt_train.iterrows():\n",
    "    for file in os.listdir(os.path.join(train_dir_path, f\"images/{row['path']}\")):\n",
    "        #print(get_label(file, row['gender'], row['age']))\n",
    "        dt_train.at[index, 'label'] = get_label(file, row['gender'], row['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a95f3599-8a5f-4815-91d5-673118deaced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "      <th>age_range</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>6954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>6955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>6956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>6957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>6959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  gender   race  age                    path  age_range  label\n",
       "0        1  female  Asian   45  000001_female_Asian_45          1      4\n",
       "1        2  female  Asian   52  000002_female_Asian_52          1      4\n",
       "2        4    male  Asian   54    000004_male_Asian_54          1      1\n",
       "3        5  female  Asian   58  000005_female_Asian_58          1      4\n",
       "4        6  female  Asian   59  000006_female_Asian_59          1      4\n",
       "...    ...     ...    ...  ...                     ...        ...    ...\n",
       "2695  6954    male  Asian   19    006954_male_Asian_19          0      0\n",
       "2696  6955    male  Asian   19    006955_male_Asian_19          0      0\n",
       "2697  6956    male  Asian   19    006956_male_Asian_19          0      0\n",
       "2698  6957    male  Asian   20    006957_male_Asian_20          0      0\n",
       "2699  6959    male  Asian   19    006959_male_Asian_19          0      0\n",
       "\n",
       "[2700 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c164652-b2c8-4340-b55d-6ba9108ad9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_idx, valid_idx = train_test_split(np.arange(len(dt_train)),\n",
    "#                                        test_size=0.2,\n",
    "#                                        shuffle=True,\n",
    "#                                        stratify=dt_train['age_range'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c53d195f-6e0c-4af7-a558-30ad3481e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_image = []\n",
    "# train_label = []\n",
    "\n",
    "# for idx in train_idx:\n",
    "#     path = dt_train.iloc[idx]['path']\n",
    "#     for file_name in [i for i in os.listdir(train_image_path+path) if i[0] != '.']:\n",
    "#         train_image.append(train_image_path+path+'/'+file_name)\n",
    "#         train_label.append((path.split('_')[1], path.split('_')[3], file_name.split('.')[0]))                                 \n",
    "\n",
    "# print(train_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d5b59fc-3b23-4b0a-a747-a3197958daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_image = []\n",
    "# valid_label = []\n",
    "\n",
    "# for idx in valid_idx:\n",
    "#     path = dt_train.iloc[idx]['path']\n",
    "#     for file_name in [i for i in os.listdir(train_image_path+path) if i[0] != '.']:\n",
    "#         valid_image.append(train_image_path+path+'/'+file_name)\n",
    "#         valid_label.append((path.split('_')[1], path.split('_')[3], file_name.split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99ee24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def get_distribution(y_vals):\n",
    "    y_distr = Counter(y_vals)\n",
    "    y_vals_sum = sum(y_distr.values())\n",
    "    return [f'{y_distr[i] / y_vals_sum:.2%}' for i in range(np.max(y_vals) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6485a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Training set : 2025, Test set : 675 \n",
      "-------------------------------------------------------------\n",
      "Fold 1, Training set : 2025, Test set : 675 \n",
      "-------------------------------------------------------------\n",
      "Fold 2, Training set : 2025, Test set : 675 \n",
      "-------------------------------------------------------------\n",
      "Fold 3, Training set : 2025, Test set : 675 \n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "groups = np.array(dt_train.id.values)\n",
    "\n",
    "distrs = [get_distribution(dt_train.label)]\n",
    "index = ['training set']\n",
    "\n",
    "train_idx, valid_idx = [], []\n",
    "train_groups, valid_groups = [], []\n",
    "train_y, valid_y = [], []\n",
    "num_fold = 4\n",
    "\n",
    "# 4개의 폴드 세트로 분리하는 StratifiedKFold 세트 생성\n",
    "skfold = StratifiedKFold(n_splits=num_fold, shuffle=True, random_state=2022)\n",
    "\n",
    "for fold_index, (train_index, valid_index) in enumerate(skfold.split(X=dt_train.id, y=dt_train.label)):\n",
    "    # print(\"%s %s\" % (train_index, test_index))\n",
    "    print(f\"Fold {fold_index}, Training set : {len(train_index)}, Test set : {len(valid_index)} \")\n",
    "    train_idx.append(train_index)\n",
    "    valid_idx.append(valid_index)\n",
    "    train_y.append(dt_train.label[train_index])\n",
    "    valid_y.append(dt_train.label[valid_index])\n",
    "    train_groups.append(groups[train_index])\n",
    "    valid_groups.append(groups[valid_index])\n",
    "\n",
    "    distrs.append(get_distribution(train_y[-1]))\n",
    "    index.append(f\"Fold {fold_index} - Training set\")\n",
    "    distrs.append(get_distribution(valid_y[-1]))\n",
    "    index.append(f\"Fold {fold_index} - Testing set\")\n",
    "\n",
    "    # print(\"train_groups: \", train_groups)\n",
    "    # print(\"train_y: \",train_y)\n",
    "    # print(\"test_groups: \",valid_groups)\n",
    "    # print(\"test_y: \",valid_y)\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5acb491c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution per class:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label 0</th>\n",
       "      <th>Label 1</th>\n",
       "      <th>Label 2</th>\n",
       "      <th>Label 3</th>\n",
       "      <th>Label 4</th>\n",
       "      <th>Label 5</th>\n",
       "      <th>Label 6</th>\n",
       "      <th>Label 7</th>\n",
       "      <th>Label 8</th>\n",
       "      <th>Label 9</th>\n",
       "      <th>Label 10</th>\n",
       "      <th>Label 11</th>\n",
       "      <th>Label 12</th>\n",
       "      <th>Label 13</th>\n",
       "      <th>Label 14</th>\n",
       "      <th>Label 15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>training set</th>\n",
       "      <td>17.59%</td>\n",
       "      <td>14.78%</td>\n",
       "      <td>3.52%</td>\n",
       "      <td>25.30%</td>\n",
       "      <td>30.00%</td>\n",
       "      <td>4.22%</td>\n",
       "      <td>1.19%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.70%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>1.81%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 0 - Training set</th>\n",
       "      <td>17.63%</td>\n",
       "      <td>14.77%</td>\n",
       "      <td>3.51%</td>\n",
       "      <td>25.28%</td>\n",
       "      <td>29.98%</td>\n",
       "      <td>4.25%</td>\n",
       "      <td>1.19%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.74%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>1.78%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 0 - Testing set</th>\n",
       "      <td>17.48%</td>\n",
       "      <td>14.81%</td>\n",
       "      <td>3.56%</td>\n",
       "      <td>25.33%</td>\n",
       "      <td>30.07%</td>\n",
       "      <td>4.15%</td>\n",
       "      <td>1.19%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.59%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>1.93%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 1 - Training set</th>\n",
       "      <td>17.58%</td>\n",
       "      <td>14.81%</td>\n",
       "      <td>3.56%</td>\n",
       "      <td>25.28%</td>\n",
       "      <td>29.98%</td>\n",
       "      <td>4.20%</td>\n",
       "      <td>1.19%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.69%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>1.83%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 1 - Testing set</th>\n",
       "      <td>17.63%</td>\n",
       "      <td>14.67%</td>\n",
       "      <td>3.41%</td>\n",
       "      <td>25.33%</td>\n",
       "      <td>30.07%</td>\n",
       "      <td>4.30%</td>\n",
       "      <td>1.19%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.74%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>1.78%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 2 - Training set</th>\n",
       "      <td>17.58%</td>\n",
       "      <td>14.77%</td>\n",
       "      <td>3.51%</td>\n",
       "      <td>25.33%</td>\n",
       "      <td>30.02%</td>\n",
       "      <td>4.20%</td>\n",
       "      <td>1.19%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.69%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>1.83%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 2 - Testing set</th>\n",
       "      <td>17.63%</td>\n",
       "      <td>14.81%</td>\n",
       "      <td>3.56%</td>\n",
       "      <td>25.19%</td>\n",
       "      <td>29.93%</td>\n",
       "      <td>4.30%</td>\n",
       "      <td>1.19%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.74%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>1.78%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 3 - Training set</th>\n",
       "      <td>17.58%</td>\n",
       "      <td>14.77%</td>\n",
       "      <td>3.51%</td>\n",
       "      <td>25.28%</td>\n",
       "      <td>30.02%</td>\n",
       "      <td>4.25%</td>\n",
       "      <td>1.19%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.69%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>1.83%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 3 - Testing set</th>\n",
       "      <td>17.63%</td>\n",
       "      <td>14.81%</td>\n",
       "      <td>3.56%</td>\n",
       "      <td>25.33%</td>\n",
       "      <td>29.93%</td>\n",
       "      <td>4.15%</td>\n",
       "      <td>1.19%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.74%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>1.78%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.89%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Label 0 Label 1 Label 2 Label 3 Label 4 Label 5 Label 6  \\\n",
       "training set           17.59%  14.78%   3.52%  25.30%  30.00%   4.22%   1.19%   \n",
       "Fold 0 - Training set  17.63%  14.77%   3.51%  25.28%  29.98%   4.25%   1.19%   \n",
       "Fold 0 - Testing set   17.48%  14.81%   3.56%  25.33%  30.07%   4.15%   1.19%   \n",
       "Fold 1 - Training set  17.58%  14.81%   3.56%  25.28%  29.98%   4.20%   1.19%   \n",
       "Fold 1 - Testing set   17.63%  14.67%   3.41%  25.33%  30.07%   4.30%   1.19%   \n",
       "Fold 2 - Training set  17.58%  14.77%   3.51%  25.33%  30.02%   4.20%   1.19%   \n",
       "Fold 2 - Testing set   17.63%  14.81%   3.56%  25.19%  29.93%   4.30%   1.19%   \n",
       "Fold 3 - Training set  17.58%  14.77%   3.51%  25.28%  30.02%   4.25%   1.19%   \n",
       "Fold 3 - Testing set   17.63%  14.81%   3.56%  25.33%  29.93%   4.15%   1.19%   \n",
       "\n",
       "                      Label 7 Label 8 Label 9 Label 10 Label 11 Label 12  \\\n",
       "training set            0.00%   0.00%   0.70%    0.00%    0.00%    1.81%   \n",
       "Fold 0 - Training set   0.00%   0.00%   0.74%    0.00%    0.00%    1.78%   \n",
       "Fold 0 - Testing set    0.00%   0.00%   0.59%    0.00%    0.00%    1.93%   \n",
       "Fold 1 - Training set   0.00%   0.00%   0.69%    0.00%    0.00%    1.83%   \n",
       "Fold 1 - Testing set    0.00%   0.00%   0.74%    0.00%    0.00%    1.78%   \n",
       "Fold 2 - Training set   0.00%   0.00%   0.69%    0.00%    0.00%    1.83%   \n",
       "Fold 2 - Testing set    0.00%   0.00%   0.74%    0.00%    0.00%    1.78%   \n",
       "Fold 3 - Training set   0.00%   0.00%   0.69%    0.00%    0.00%    1.83%   \n",
       "Fold 3 - Testing set    0.00%   0.00%   0.74%    0.00%    0.00%    1.78%   \n",
       "\n",
       "                      Label 13 Label 14 Label 15  \n",
       "training set             0.00%    0.00%    0.89%  \n",
       "Fold 0 - Training set    0.00%    0.00%    0.89%  \n",
       "Fold 0 - Testing set     0.00%    0.00%    0.89%  \n",
       "Fold 1 - Training set    0.00%    0.00%    0.89%  \n",
       "Fold 1 - Testing set     0.00%    0.00%    0.89%  \n",
       "Fold 2 - Training set    0.00%    0.00%    0.89%  \n",
       "Fold 2 - Testing set     0.00%    0.00%    0.89%  \n",
       "Fold 3 - Training set    0.00%    0.00%    0.89%  \n",
       "Fold 3 - Testing set     0.00%    0.00%    0.89%  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Distribution per class:')\n",
    "pd.DataFrame(distrs, index=index, columns=[f'Label {l}' for l in range(np.max(train_y) + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98de1eb4-55d4-41ba-8541-70fff9bb011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_enc(x):\n",
    "    def gender(i):\n",
    "        if i == 'male':\n",
    "            return 0\n",
    "        elif i == 'female':\n",
    "            return 3\n",
    "    def age(j):\n",
    "        j = int(j)\n",
    "        if j < 30:\n",
    "            return 0\n",
    "        elif j >= 30 and j < 60:\n",
    "            return 1\n",
    "        elif j >= 60:\n",
    "            return 2\n",
    "    def mask(k):\n",
    "        if k == 'normal':\n",
    "            return 12\n",
    "        elif 'incorrect' in k:\n",
    "            return 6\n",
    "        else:\n",
    "            return 0\n",
    "    return gender(x[0]) + age(x[1]) + mask(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "943d0388-3736-4b6b-a624-0213148616f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.Series(train_image)\n",
    "# train_label = pd.Series(train_label)\n",
    "\n",
    "# valid_data = pd.Series(valid_image)\n",
    "# valid_label = pd.Series(valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "551c673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Mask(Dataset):\n",
    "    def __init__(self, data, label, encoding=True, midcrop=True, transform=None):\n",
    "        self.encoding = encoding\n",
    "        self.midcrop = midcrop\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        \n",
    "        if encoding:\n",
    "            self.label = self.label.apply(onehot_enc)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        y = self.label[idx]\n",
    "        \n",
    "        if self.midcrop:\n",
    "            X = X[70:420, 17:367]\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(X), y\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c0e4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_class_weights(labels, class_num):\n",
    "    # print(\"Labels shape:\\n\", labels.shape)\n",
    "    # print(\"Given labels:\\n\", labels)\n",
    "\n",
    "    labels = labels.apply(lambda x : x % class_num)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    class_weights = np.zeros_like(labels) \n",
    "    \n",
    "    _, counts = np.unique(labels, return_counts=True)\n",
    "    # 각 class가 몇 번 등장하는지 count\n",
    "\n",
    "    # print(\"Labels:\\n\", labels)\n",
    "    # print(\"Label count:\\n\", counts)\n",
    "\n",
    "    for cls in range(class_num):\n",
    "        class_weights = np.where(labels == cls, 1/counts[cls], class_weights)\n",
    "        # label이 class에 해당하면 count의 역수 적용\n",
    "    return class_weights\n",
    "\n",
    "class_num = 18\n",
    "age_class_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fe3b5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필요 입력 채널 개수 3\n",
      "네트워크 출력 채널 개수 1000\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "print('필요 입력 채널 개수', model.conv1.weight.shape[1])\n",
    "print('네트워크 출력 채널 개수', model.fc.weight.shape[0])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9899a97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필요 입력 채널 개수 3\n",
      "네트워크 출력 채널 개수 18\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "model.fc = nn.Linear(in_features=2048, out_features=class_num, bias=True)\n",
    "nn.init.xavier_uniform_(model.fc.weight)\n",
    "stdv = 1. / math.sqrt(model.fc.weight.size(1))\n",
    "model.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "print('필요 입력 채널 개수', model.conv1.weight.shape[1])\n",
    "print('네트워크 출력 채널 개수', model.fc.weight.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7cc905d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCH = 10\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b82bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "best_f1_score = 0\n",
    "\n",
    "def train(fold_idx, train_dataloader_mask, val_dataloader_mask):\n",
    "    patience = 3\n",
    "    cur_count = 0\n",
    "\n",
    "    cur_val_acc = best_val_acc\n",
    "    cur_val_loss = best_val_loss\n",
    "    cur_f1_score = best_f1_score\n",
    "\n",
    "    f1 = F1Score(num_classes=class_num, average='macro').to(device)\n",
    "\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        model.train()\n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "        for train_batch in tqdm(train_dataloader_mask):\n",
    "            inputs, labels = train_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outs = model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            loss = criterion(outs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                torch.save(model, '/opt/ml/checkpoint/resnet50_kfold/checkpoint_%dfold_ep%d.pt'%(fold_idx, epoch))\n",
    "            \n",
    "            loss_value += loss.item()\n",
    "            matches += (preds == labels).sum().item()\n",
    "\n",
    "        train_loss = loss_value / batch_size\n",
    "        train_acc = matches / batch_size\n",
    "\n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "\n",
    "        print(f\"epoch[{epoch}/{NUM_EPOCH}] training loss {train_loss:.3f}, training accuracy {train_acc:.3f}\")\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss_items = []\n",
    "            val_acc_items = []\n",
    "            f1_score = 0\n",
    "            for val_batch in val_dataloader_mask:\n",
    "                inputs, labels = val_batch\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outs = model(inputs)\n",
    "                preds = torch.argmax(outs, dim=-1)\n",
    "                \n",
    "                loss_item = criterion(outs, labels).item()\n",
    "                acc_item = (labels==preds).sum().item()\n",
    "                val_loss_items.append(loss_item)\n",
    "                val_acc_items.append(acc_item)\n",
    "                f1_score += f1(outs, labels)\n",
    "                \n",
    "            val_loss = np.sum(val_loss_items) / len(val_dataloader_mask)\n",
    "            val_acc = np.sum(val_acc_items) / len(val_dataloader_mask.dataset)\n",
    "            f1_score /= len(val_dataloader_mask)\n",
    "            \n",
    "            if val_loss < cur_val_loss:\n",
    "                cur_val_loss = val_loss\n",
    "\n",
    "            if val_acc > cur_val_acc:\n",
    "                cur_val_acc = val_acc\n",
    "\n",
    "            if f1_score > cur_f1_score:\n",
    "                cur_f1_score = f1_score\n",
    "                cur_count = 0\n",
    "                torch.save(model, '/opt/ml/checkpoint/resnet50_kfold/checkpoint_best.pt')\n",
    "                print(\"Update checkpoint!!!\")\n",
    "            else:\n",
    "                cur_count += 1\n",
    "                if cur_count >= patience:\n",
    "                    print(\"Early Stopping!\")\n",
    "                    break\n",
    "            print(f\"[val] acc : {val_acc:.3f}, loss : {val_loss:.3f}, f1 score: {f1_score:.3f}\")\n",
    "            print(f\"best acc : {cur_val_acc:.3f}, best loss : {cur_val_loss:.3f}, best f1 : {cur_f1_score:.3f}\")\n",
    "\n",
    "    return cur_val_acc, cur_val_loss, cur_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "440c4706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** 0 th fold********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [04:00<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[0/10] training loss 1.232, training accuracy 197.859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/222 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.880, loss : 0.389, f1 score: 0.758\n",
      "best acc : 0.880, best loss : 0.389, best f1 : 0.758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [02:13<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1/10] training loss 0.179, training accuracy 218.109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/222 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] acc : 0.854, loss : 0.606, f1 score: 0.735\n",
      "best acc : 0.880, best loss : 0.389, best f1 : 0.758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [02:13<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[2/10] training loss 0.083, training accuracy 220.047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/222 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] acc : 0.871, loss : 0.591, f1 score: 0.755\n",
      "best acc : 0.880, best loss : 0.389, best f1 : 0.758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [02:13<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[3/10] training loss 0.095, training accuracy 219.578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping!\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "0'th FOLD - best acc : 0.880, best loss : 0.389, best f1 : 0.758\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "******** 1 th fold********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [03:58<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[0/10] training loss 0.303, training accuracy 215.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/222 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.960, loss : 0.110, f1 score: 0.928\n",
      "best acc : 0.960, best loss : 0.110, best f1 : 0.928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [02:13<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1/10] training loss 0.063, training accuracy 220.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/222 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.979, loss : 0.057, f1 score: 0.947\n",
      "best acc : 0.979, best loss : 0.057, best f1 : 0.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [02:13<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[2/10] training loss 0.029, training accuracy 221.062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/222 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.981, loss : 0.050, f1 score: 0.956\n",
      "best acc : 0.981, best loss : 0.050, best f1 : 0.956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [02:13<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[3/10] training loss 0.017, training accuracy 221.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/222 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.977, loss : 0.070, f1 score: 0.959\n",
      "best acc : 0.981, best loss : 0.050, best f1 : 0.959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [02:13<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[4/10] training loss 0.016, training accuracy 221.219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/222 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] acc : 0.963, loss : 0.129, f1 score: 0.943\n",
      "best acc : 0.981, best loss : 0.050, best f1 : 0.959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 14/222 [00:15<03:47,  1.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-13db144fbf55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# print(len(val_dataloader_mask))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mcur_val_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_f1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mbest_val_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcur_val_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-fe2f5cd9e9e9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(fold_idx, train_dataloader_mask, val_dataloader_mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             adam(params_with_grad,\n\u001b[0m\u001b[1;32m    235\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    301\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapturable\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold_idx in range(num_fold):\n",
    "    print(\"********\", fold_idx,\"th fold********\")\n",
    "    train_image = []\n",
    "    train_label = []\n",
    "    for idx in train_idx[fold_idx]:\n",
    "        path = dt_train.iloc[idx]['path']\n",
    "        for file_name in [i for i in os.listdir(train_image_path+path) if i[0] != '.']:\n",
    "            train_image.append(train_image_path+path+'/'+file_name)\n",
    "            train_label.append((path.split('_')[1], path.split('_')[3], file_name.split('.')[0])) \n",
    "\n",
    "    valid_image = []\n",
    "    valid_label = []\n",
    "    for idx in valid_idx[fold_idx]:\n",
    "        path = dt_train.iloc[idx]['path']\n",
    "        for file_name in [i for i in os.listdir(train_image_path+path) if i[0] != '.']:\n",
    "            valid_image.append(train_image_path+path+'/'+file_name)\n",
    "            valid_label.append((path.split('_')[1], path.split('_')[3], file_name.split('.')[0]))                                 \n",
    "\n",
    "    train_data = pd.Series(train_image)\n",
    "    train_label = pd.Series(train_label)\n",
    "    valid_data = pd.Series(valid_image)\n",
    "    valid_label = pd.Series(valid_label)\n",
    "\n",
    "    mask_train_set = Dataset_Mask(data=train_data, label=train_label, transform = transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))\n",
    "\n",
    "    mask_val_set = Dataset_Mask(data=valid_data, label=valid_label, transform = transforms.Compose([\n",
    "                                    transforms.ToTensor()\n",
    "                                ]))\n",
    "\n",
    "    # print(f'training data size : {len(mask_train_set)}')\n",
    "    # print(f'validation data size : {len(mask_val_set)}')\n",
    "    \n",
    "    class_weights = make_class_weights(mask_train_set.label, age_class_num)\n",
    "\n",
    "    #sampler = sampler.WeightedRandomSampler(weights=class_weights, num_samples=len(class_weights))\n",
    "\n",
    "    #train_dataloader_mask = DataLoader(dataset = mask_train_set, batch_size=batch_size, sampler=sampler, num_workers=2)\n",
    "    train_dataloader_mask = DataLoader(dataset = mask_train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_dataloader_mask = DataLoader(dataset = mask_val_set, batch_size=batch_size, shuffle=True,  num_workers=2)\n",
    "\n",
    "    # print(len(train_dataloader_mask))\n",
    "    # print(len(val_dataloader_mask))\n",
    "\n",
    "    cur_val_acc, cur_val_loss, cur_f1_score = train(fold_idx, train_dataloader_mask, val_dataloader_mask)\n",
    "\n",
    "    best_val_acc += cur_val_acc\n",
    "    best_val_loss += cur_val_loss\n",
    "    best_f1_score += cur_f1_score\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"{fold_idx}'th FOLD - best acc : {cur_val_acc:.3f}, best loss : {cur_val_loss:.3f}, best f1 : {cur_f1_score:.3f}\")\n",
    "    print(\"--------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "best_val_acc /= num_fold\n",
    "best_val_loss /= num_fold\n",
    "best_f1_score /= num_fold\n",
    "\n",
    "print(f\"****FINAL - best acc : {best_val_acc:.3f}, best loss : {best_val_loss:.3f}, best f1 : {best_f1_score:.3f}*****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a682328",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 4\n",
    "best_val_acc /= temp\n",
    "best_val_loss /= temp\n",
    "best_f1_score /= temp\n",
    "\n",
    "print(f\"****FINAL - best acc : {best_val_acc:.3f}, best loss : {best_val_loss:.3f}, best f1 : {best_f1_score:.3f}*****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e72a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665b7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afda23d0-6ad9-408b-99c9-705ae5fd8347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbc5c6e168e63498590db46022617123f1fe1268.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0e72482bf56b3581c081f7da2a6180b8792c7089.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b549040c49190cedc41327748aeb197c1670f14d.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ImageID  ans\n",
       "0  cbc5c6e168e63498590db46022617123f1fe1268.jpg    0\n",
       "1  0e72482bf56b3581c081f7da2a6180b8792c7089.jpg    0\n",
       "2  b549040c49190cedc41327748aeb197c1670f14d.jpg    0\n",
       "3  4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg    0\n",
       "4  248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg    0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "test_dir_path = '/opt/ml/input/data/eval/'\n",
    "test_image_path = '/opt/ml/input/data/eval/images/'\n",
    "\n",
    "model = torch.load('/opt/ml/checkpoint/resnet50_kfold/checkpoint_best.pt')\n",
    "submission = pd.read_csv(test_dir_path+'info.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "566c2df1-a083-4e27-a655-f2c73d15f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [os.path.join(test_image_path, img_id) for img_id in submission.ImageID]\n",
    "test_image = pd.Series(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9c14322-45a8-40a5-aded-f335204484b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Dataset(Dataset):\n",
    "    def __init__(self, midcrop=True, transform=None):\n",
    "        self.midcrop = midcrop\n",
    "        self.data = test_image\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(test_image)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.midcrop:\n",
    "            img = img[64:448]\n",
    "            \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "138db932-cc09-4f85-89d8-e7485659f7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:44<00:00,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = Test_Dataset(transform = transforms.Compose([\n",
    "                            transforms.ToTensor()\n",
    "                        ]))\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in tqdm(loader):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir_path, 'submission_resnet50_kfold.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
