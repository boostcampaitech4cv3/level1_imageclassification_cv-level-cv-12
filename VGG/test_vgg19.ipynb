{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>006954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>006955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>006956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>006957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path\n",
       "0     000001  female  Asian   45  000001_female_Asian_45\n",
       "1     000002  female  Asian   52  000002_female_Asian_52\n",
       "2     000004    male  Asian   54    000004_male_Asian_54\n",
       "3     000005  female  Asian   58  000005_female_Asian_58\n",
       "4     000006  female  Asian   59  000006_female_Asian_59\n",
       "...      ...     ...    ...  ...                     ...\n",
       "2695  006954    male  Asian   19    006954_male_Asian_19\n",
       "2696  006955    male  Asian   19    006955_male_Asian_19\n",
       "2697  006956    male  Asian   19    006956_male_Asian_19\n",
       "2698  006957    male  Asian   20    006957_male_Asian_20\n",
       "2699  006959    male  Asian   19    006959_male_Asian_19\n",
       "\n",
       "[2700 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = '/opt/ml/input/data/train/'\n",
    "train_image_path = '/opt/ml/input/data/train/images/'\n",
    "\n",
    "test_dir = '/opt/ml/input/data/eval/'\n",
    "test_image_path = '/opt/ml/input/data/eval/images/'\n",
    "\n",
    "dt_train = pd.read_csv(train_dir+'train.csv')\n",
    "dt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7724d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8031f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 12\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012a2b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일단은 list로 값 받는 형태로 _ Dataset 안에서 처리할 수도 있을듯\n",
    "whole_image_path = []\n",
    "whole_target_label = []\n",
    "\n",
    "for path in dt_train['path']:\n",
    "    for file_name in [i for i in os.listdir(train_image_path+path) if '._' not in i]:\n",
    "        whole_image_path.append(train_image_path+path+'/'+file_name)\n",
    "        whole_target_label.append((path.split('_')[1], path.split('_')[3], file_name.split('.')[0]))\n",
    "        \n",
    "        \n",
    "# 라벨을 0~17로 할당하는 함수\n",
    "def onehot_enc(x):\n",
    "    # x 입력형태 : (gender, age, mask)의 튜플\n",
    "    def gender(i):\n",
    "        if i == 'male':\n",
    "            return 0\n",
    "        elif i == 'female':\n",
    "            return 3\n",
    "    def age(j):\n",
    "        j = int(j)\n",
    "        if j < 30:\n",
    "            return 0\n",
    "        elif j >= 30 and j < 60:\n",
    "            return 1\n",
    "        elif j >= 60:\n",
    "            return 2\n",
    "    def mask(k):\n",
    "        if k == 'normal':\n",
    "            return 12\n",
    "        elif 'incorrect' in k:\n",
    "            return 6\n",
    "        else:\n",
    "            return 0\n",
    "    return gender(x[0]) + age(x[1]) + mask(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ffeefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sr_data : 이미지 데이터의 경로\n",
    "# sr_label : 이미지 데이터의 라벨정보 (not_encoded)\n",
    "sr_data = pd.Series(whole_image_path)\n",
    "sr_label = pd.Series(whole_target_label)\n",
    "#64, 447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Mask(Dataset):\n",
    "    def __init__(self, encoding=True, midcrop=True, transform=None):\n",
    "        self.encoding = encoding\n",
    "        self.midcrop = midcrop\n",
    "        self.data = sr_data\n",
    "        self.label = sr_label\n",
    "        self.transform = transform\n",
    "        \n",
    "        if encoding:\n",
    "            self.label = self.label.apply(onehot_enc)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(sr_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        y = self.label[idx]\n",
    "        \n",
    "        if self.midcrop:\n",
    "            X = X[64:448]\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(X), y\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fd6db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mask = Dataset_Mask(transform = transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df62b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size : 15120\n",
      "validation data size : 3780\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(dataset_mask) * 0.8)\n",
    "val_size = int(len(dataset_mask) * 0.2)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(dataset_mask, [train_size, val_size])\n",
    "print(f'training data size : {len(train_set)}')\n",
    "print(f'validation data size : {len(val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc8f20fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataLoader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dataLoader = DataLoader(dataset = val_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2845076e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_classes = 18\n",
    "model_vgg19 = models.vgg19(pretrained=True)\n",
    "\n",
    "print(model_vgg19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9acfbe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=18, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_vgg19.classifier = nn.Sequential(\n",
    "    nn.Linear(512 * 7 * 7, 4096),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(4096, num_classes),\n",
    ")\n",
    "\n",
    "print(model_vgg19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b17965d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.weight.data.normal_(0, 0.01)\n",
    "            m.bias.data.zero_()\n",
    "\n",
    "initialize_weights(model_vgg19.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb26e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.0001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_vgg19.parameters(), lr=learning_rate, betas=(0.9,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6f7478d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=18, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vgg19.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c88981f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] name:[features.0.weight] shape:[(64, 3, 3, 3)].\n",
      "    val:[-0.053 -0.049 -0.068  0.015  0.045]\n",
      "[1] name:[features.0.bias] shape:[(64,)].\n",
      "    val:[-0.913  0.307 -1.306 -0.776 -0.789]\n",
      "[2] name:[features.2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[0.053 0.083 0.086 0.027 0.033]\n",
      "[3] name:[features.2.bias] shape:[(64,)].\n",
      "    val:[-0.058 -0.148  0.18  -0.286  0.014]\n",
      "[4] name:[features.5.weight] shape:[(128, 64, 3, 3)].\n",
      "    val:[-0.024  0.005  0.002 -0.033  0.091]\n",
      "[5] name:[features.5.bias] shape:[(128,)].\n",
      "    val:[0.025 0.133 0.008 0.05  0.031]\n",
      "[6] name:[features.7.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[ 0.015 -0.037  0.053 -0.002 -0.052]\n",
      "[7] name:[features.7.bias] shape:[(128,)].\n",
      "    val:[ 0.084 -0.082 -0.069 -0.016  0.437]\n",
      "[8] name:[features.10.weight] shape:[(256, 128, 3, 3)].\n",
      "    val:[ 0.036  0.05   0.051 -0.065 -0.061]\n",
      "[9] name:[features.10.bias] shape:[(256,)].\n",
      "    val:[-0.057  0.025  0.091 -0.321  0.012]\n",
      "[10] name:[features.12.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.022  0.009 -0.002  0.001  0.003]\n",
      "[11] name:[features.12.bias] shape:[(256,)].\n",
      "    val:[-0.16  -0.051 -0.037  0.156  0.063]\n",
      "[12] name:[features.14.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.022 -0.008  0.01  -0.014 -0.007]\n",
      "[13] name:[features.14.bias] shape:[(256,)].\n",
      "    val:[ 2.322e-01 -2.414e-04  6.496e-02 -4.202e-01  6.045e-02]\n",
      "[14] name:[features.16.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.02   0.046  0.002 -0.011  0.005]\n",
      "[15] name:[features.16.bias] shape:[(256,)].\n",
      "    val:[ 0.111 -0.066  0.117  0.103  0.138]\n",
      "[16] name:[features.19.weight] shape:[(512, 256, 3, 3)].\n",
      "    val:[0.008 0.016 0.005 0.003 0.018]\n",
      "[17] name:[features.19.bias] shape:[(512,)].\n",
      "    val:[ 0.049  0.185  0.01  -0.052  0.028]\n",
      "[18] name:[features.21.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[-0.063 -0.026  0.002 -0.065 -0.025]\n",
      "[19] name:[features.21.bias] shape:[(512,)].\n",
      "    val:[ 0.064 -0.011 -0.032  0.216  0.015]\n",
      "[20] name:[features.23.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[ 0.003 -0.012 -0.025  0.013  0.017]\n",
      "[21] name:[features.23.bias] shape:[(512,)].\n",
      "    val:[ 0.094  0.029 -0.038  0.136  0.004]\n",
      "[22] name:[features.25.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[0.005 0.015 0.016 0.002 0.005]\n",
      "[23] name:[features.25.bias] shape:[(512,)].\n",
      "    val:[0.055 0.09  0.138 0.153 0.087]\n",
      "[24] name:[features.28.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[0.011 0.007 0.018 0.01  0.008]\n",
      "[25] name:[features.28.bias] shape:[(512,)].\n",
      "    val:[-0.052  0.101  0.081  0.015  0.063]\n",
      "[26] name:[features.30.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[-0.01  -0.016 -0.011  0.007  0.016]\n",
      "[27] name:[features.30.bias] shape:[(512,)].\n",
      "    val:[ 0.025 -0.032  0.009  0.237  0.039]\n",
      "[28] name:[features.32.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[-0.013 -0.009 -0.014 -0.003 -0.013]\n",
      "[29] name:[features.32.bias] shape:[(512,)].\n",
      "    val:[-0.021  0.064  0.027 -0.236 -0.018]\n",
      "[30] name:[features.34.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[ 0.006 -0.006 -0.006  0.006 -0.005]\n",
      "[31] name:[features.34.bias] shape:[(512,)].\n",
      "    val:[ 0.157 -0.007  0.052 -0.026 -0.06 ]\n",
      "[32] name:[classifier.0.weight] shape:[(4096, 25088)].\n",
      "    val:[ 0.004 -0.003 -0.013 -0.007  0.004]\n",
      "[33] name:[classifier.0.bias] shape:[(4096,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[34] name:[classifier.3.weight] shape:[(4096, 4096)].\n",
      "    val:[0.002 0.006 0.005 0.004 0.011]\n",
      "[35] name:[classifier.3.bias] shape:[(4096,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[36] name:[classifier.6.weight] shape:[(18, 4096)].\n",
      "    val:[0.007 0.021 0.005 0.    0.008]\n",
      "[37] name:[classifier.6.bias] shape:[(18,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "Total number of parameters:[139,643,986].\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "n_param = 0\n",
    "for p_idx, (param_name, param) in enumerate(model_vgg19.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        param_numpy = param.detach().cpu().numpy()\n",
    "        n_param += len(param_numpy.reshape(-1))\n",
    "        print (\"[%d] name:[%s] shape:[%s].\"%(p_idx,param_name,param_numpy.shape))\n",
    "        print (\"    val:%s\"%(param_numpy.reshape(-1)[:5]))\n",
    "print (\"Total number of parameters:[%s].\"%(format(n_param,',d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9ed1f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training Epoch : 0\n",
      "STEP[0/237] Training loss: 0.046 / Training accuracy: 0.047\n",
      "STEP[10/237] Training loss: 0.029 / Training accuracy: 0.312\n",
      "STEP[20/237] Training loss: 0.020 / Training accuracy: 0.609\n",
      "STEP[30/237] Training loss: 0.018 / Training accuracy: 0.641\n",
      "STEP[40/237] Training loss: 0.011 / Training accuracy: 0.781\n",
      "STEP[50/237] Training loss: 0.011 / Training accuracy: 0.781\n",
      "STEP[60/237] Training loss: 0.012 / Training accuracy: 0.781\n",
      "STEP[70/237] Training loss: 0.011 / Training accuracy: 0.797\n",
      "STEP[80/237] Training loss: 0.010 / Training accuracy: 0.766\n",
      "STEP[90/237] Training loss: 0.007 / Training accuracy: 0.859\n",
      "STEP[100/237] Training loss: 0.008 / Training accuracy: 0.828\n",
      "STEP[110/237] Training loss: 0.008 / Training accuracy: 0.812\n",
      "STEP[120/237] Training loss: 0.008 / Training accuracy: 0.875\n",
      "STEP[130/237] Training loss: 0.007 / Training accuracy: 0.891\n",
      "STEP[140/237] Training loss: 0.008 / Training accuracy: 0.812\n",
      "STEP[150/237] Training loss: 0.007 / Training accuracy: 0.875\n",
      "STEP[160/237] Training loss: 0.008 / Training accuracy: 0.844\n",
      "STEP[170/237] Training loss: 0.006 / Training accuracy: 0.922\n",
      "STEP[180/237] Training loss: 0.009 / Training accuracy: 0.812\n",
      "STEP[190/237] Training loss: 0.004 / Training accuracy: 0.906\n",
      "STEP[200/237] Training loss: 0.006 / Training accuracy: 0.891\n",
      "STEP[210/237] Training loss: 0.006 / Training accuracy: 0.844\n",
      "STEP[220/237] Training loss: 0.005 / Training accuracy: 0.938\n",
      "STEP[230/237] Training loss: 0.005 / Training accuracy: 0.844\n",
      "[Val] Current acc : 56.117 / Current loss : 0.312\n",
      "[Val] Best acc : 56.117 / Best loss : 0.312\n",
      "EPOCH[0/10] Training loss: 0.003 / Training accuracy: 0.234\n",
      "[INFO] Training Epoch : 1\n",
      "STEP[0/237] Training loss: 0.003 / Training accuracy: 0.938\n",
      "STEP[10/237] Training loss: 0.005 / Training accuracy: 0.906\n",
      "STEP[20/237] Training loss: 0.007 / Training accuracy: 0.859\n",
      "STEP[30/237] Training loss: 0.004 / Training accuracy: 0.938\n",
      "STEP[40/237] Training loss: 0.005 / Training accuracy: 0.906\n",
      "STEP[50/237] Training loss: 0.002 / Training accuracy: 0.984\n",
      "STEP[60/237] Training loss: 0.004 / Training accuracy: 0.906\n",
      "STEP[70/237] Training loss: 0.003 / Training accuracy: 0.938\n",
      "STEP[80/237] Training loss: 0.005 / Training accuracy: 0.859\n",
      "STEP[90/237] Training loss: 0.005 / Training accuracy: 0.859\n",
      "STEP[100/237] Training loss: 0.003 / Training accuracy: 0.922\n",
      "STEP[110/237] Training loss: 0.003 / Training accuracy: 0.969\n",
      "STEP[120/237] Training loss: 0.004 / Training accuracy: 0.938\n",
      "STEP[130/237] Training loss: 0.002 / Training accuracy: 0.938\n",
      "STEP[140/237] Training loss: 0.002 / Training accuracy: 0.938\n",
      "STEP[150/237] Training loss: 0.003 / Training accuracy: 0.922\n",
      "STEP[160/237] Training loss: 0.003 / Training accuracy: 0.938\n",
      "STEP[170/237] Training loss: 0.002 / Training accuracy: 0.953\n",
      "STEP[180/237] Training loss: 0.002 / Training accuracy: 0.938\n",
      "STEP[190/237] Training loss: 0.004 / Training accuracy: 0.922\n",
      "STEP[200/237] Training loss: 0.003 / Training accuracy: 0.922\n",
      "STEP[210/237] Training loss: 0.004 / Training accuracy: 0.891\n",
      "STEP[220/237] Training loss: 0.003 / Training accuracy: 0.938\n",
      "STEP[230/237] Training loss: 0.002 / Training accuracy: 0.953\n",
      "[Val] Current acc : 59.883 / Current loss : 0.157\n",
      "[Val] Best acc : 59.883 / Best loss : 0.157\n",
      "EPOCH[1/10] Training loss: 0.003 / Training accuracy: 0.219\n",
      "[INFO] Training Epoch : 2\n",
      "STEP[0/237] Training loss: 0.001 / Training accuracy: 0.969\n",
      "STEP[10/237] Training loss: 0.002 / Training accuracy: 0.922\n",
      "STEP[20/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[30/237] Training loss: 0.003 / Training accuracy: 0.953\n",
      "STEP[40/237] Training loss: 0.001 / Training accuracy: 0.969\n",
      "STEP[50/237] Training loss: 0.001 / Training accuracy: 0.969\n",
      "STEP[60/237] Training loss: 0.002 / Training accuracy: 0.953\n",
      "STEP[70/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[80/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[90/237] Training loss: 0.002 / Training accuracy: 0.953\n",
      "STEP[100/237] Training loss: 0.002 / Training accuracy: 0.969\n",
      "STEP[110/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[120/237] Training loss: 0.002 / Training accuracy: 0.953\n",
      "STEP[130/237] Training loss: 0.004 / Training accuracy: 0.922\n",
      "STEP[140/237] Training loss: 0.003 / Training accuracy: 0.953\n",
      "STEP[150/237] Training loss: 0.002 / Training accuracy: 0.938\n",
      "STEP[160/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[170/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[180/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[190/237] Training loss: 0.002 / Training accuracy: 0.953\n",
      "STEP[200/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[210/237] Training loss: 0.001 / Training accuracy: 0.969\n",
      "STEP[220/237] Training loss: 0.002 / Training accuracy: 0.984\n",
      "STEP[230/237] Training loss: 0.001 / Training accuracy: 0.969\n",
      "[Val] Current acc : 60.983 / Current loss : 0.118\n",
      "[Val] Best acc : 60.983 / Best loss : 0.118\n",
      "EPOCH[2/10] Training loss: 0.000 / Training accuracy: 0.250\n",
      "[INFO] Training Epoch : 3\n",
      "STEP[0/237] Training loss: 0.001 / Training accuracy: 0.969\n",
      "STEP[10/237] Training loss: 0.003 / Training accuracy: 0.938\n",
      "STEP[20/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[30/237] Training loss: 0.001 / Training accuracy: 0.969\n",
      "STEP[40/237] Training loss: 0.001 / Training accuracy: 0.953\n",
      "STEP[50/237] Training loss: 0.002 / Training accuracy: 0.969\n",
      "STEP[60/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[70/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[80/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[90/237] Training loss: 0.002 / Training accuracy: 0.953\n",
      "STEP[100/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[110/237] Training loss: 0.003 / Training accuracy: 0.938\n",
      "STEP[120/237] Training loss: 0.003 / Training accuracy: 0.922\n",
      "STEP[130/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[140/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[150/237] Training loss: 0.002 / Training accuracy: 0.969\n",
      "STEP[160/237] Training loss: 0.002 / Training accuracy: 0.938\n",
      "STEP[170/237] Training loss: 0.003 / Training accuracy: 0.953\n",
      "STEP[180/237] Training loss: 0.002 / Training accuracy: 0.969\n",
      "STEP[190/237] Training loss: 0.001 / Training accuracy: 1.000\n",
      "STEP[200/237] Training loss: 0.001 / Training accuracy: 0.938\n",
      "STEP[210/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[220/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[230/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "[Val] Current acc : 61.517 / Current loss : 0.078\n",
      "[Val] Best acc : 61.517 / Best loss : 0.078\n",
      "EPOCH[3/10] Training loss: 0.000 / Training accuracy: 0.250\n",
      "[INFO] Training Epoch : 4\n",
      "STEP[0/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[10/237] Training loss: 0.002 / Training accuracy: 0.984\n",
      "STEP[20/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[30/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[40/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[50/237] Training loss: 0.000 / Training accuracy: 0.984\n",
      "STEP[60/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[70/237] Training loss: 0.002 / Training accuracy: 0.984\n",
      "STEP[80/237] Training loss: 0.001 / Training accuracy: 0.969\n",
      "STEP[90/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[100/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[110/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[120/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[130/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[140/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[150/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[160/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[170/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[180/237] Training loss: 0.001 / Training accuracy: 0.969\n",
      "STEP[190/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[200/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[210/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[220/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[230/237] Training loss: 0.003 / Training accuracy: 0.953\n",
      "[Val] Current acc : 61.517 / Current loss : 0.076\n",
      "[Val] Best acc : 61.517 / Best loss : 0.076\n",
      "EPOCH[4/10] Training loss: 0.000 / Training accuracy: 0.250\n",
      "[INFO] Training Epoch : 5\n",
      "STEP[0/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[10/237] Training loss: 0.000 / Training accuracy: 0.984\n",
      "STEP[20/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[30/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[40/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[50/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[60/237] Training loss: 0.002 / Training accuracy: 0.969\n",
      "STEP[70/237] Training loss: 0.002 / Training accuracy: 0.984\n",
      "STEP[80/237] Training loss: 0.000 / Training accuracy: 0.984\n",
      "STEP[90/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[100/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[110/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[120/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[130/237] Training loss: 0.002 / Training accuracy: 0.984\n",
      "STEP[140/237] Training loss: 0.000 / Training accuracy: 0.984\n",
      "STEP[150/237] Training loss: 0.002 / Training accuracy: 0.984\n",
      "STEP[160/237] Training loss: 0.001 / Training accuracy: 0.969\n",
      "STEP[170/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[180/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[190/237] Training loss: 0.002 / Training accuracy: 0.953\n",
      "STEP[200/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[210/237] Training loss: 0.000 / Training accuracy: 0.984\n",
      "STEP[220/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[230/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "[Val] Current acc : 61.750 / Current loss : 0.059\n",
      "[Val] Best acc : 61.750 / Best loss : 0.059\n",
      "EPOCH[5/10] Training loss: 0.000 / Training accuracy: 0.250\n",
      "[INFO] Training Epoch : 6\n",
      "STEP[0/237] Training loss: 0.001 / Training accuracy: 0.984\n",
      "STEP[10/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[20/237] Training loss: 0.000 / Training accuracy: 1.000\n",
      "STEP[30/237] Training loss: 0.002 / Training accuracy: 0.969\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-71f618151168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_acc_so_far = 0\n",
    "best_val_loss_so_far = np.inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"[INFO] Training Epoch : {}\".format(epoch))\n",
    "    model_vgg19.train()\n",
    "\n",
    "    for i, (X, y) in enumerate(train_dataLoader):\n",
    "        loss_value = 0\n",
    "        acc = 0\n",
    "        \n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        _y = model_vgg19(X)\n",
    "        _pred = torch.argmax(_y, dim=-1)\n",
    "\n",
    "        loss = criterion(_y, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        loss_value += loss.item()\n",
    "        acc += (y == _pred).sum().item()\n",
    "\n",
    "        train_loss = loss_value / batch_size\n",
    "        train_acc = acc / batch_size\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"STEP[{i}/{len(train_dataLoader)}] Training loss: {train_loss:.3f} / Training accuracy: {train_acc:.3f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_vgg19.eval()\n",
    "\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "\n",
    "        for i, (X, y) in enumerate(val_dataLoader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            _y = model_vgg19(X)\n",
    "            _pred = torch.argmax(_y, dim=-1)\n",
    "            \n",
    "            loss_item = criterion(_y, y).item()\n",
    "            acc_item = (y == _pred).sum().item()\n",
    "\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "            \n",
    "        cur_val_loss = np.sum(val_loss_items) / len(val_dataLoader)\n",
    "        cur_val_acc = np.sum(val_acc_items) / len(val_dataLoader)\n",
    "        \n",
    "        if cur_val_loss < best_val_loss_so_far:\n",
    "            best_val_loss_so_far = cur_val_loss\n",
    "        if cur_val_acc > best_val_acc_so_far:\n",
    "            best_val_acc_so_far = cur_val_acc\n",
    "\n",
    "        print(f\"[Val] Current acc : {cur_val_acc:.3f} / Current loss : {cur_val_loss:.3f}\")\n",
    "        print(f\"[Val] Best acc : {best_val_acc_so_far:.3f} / Best loss : {best_val_loss_so_far:.3f}\")\n",
    "\n",
    "    print(f\"EPOCH[{epoch}/{epochs}] Training loss: {train_loss:.3f} / Training accuracy: {train_acc:.3f}\")\n",
    "\n",
    "print(\"[INFO] Training ALL DONE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf996993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbc5c6e168e63498590db46022617123f1fe1268.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0e72482bf56b3581c081f7da2a6180b8792c7089.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b549040c49190cedc41327748aeb197c1670f14d.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ImageID  ans\n",
       "0  cbc5c6e168e63498590db46022617123f1fe1268.jpg    0\n",
       "1  0e72482bf56b3581c081f7da2a6180b8792c7089.jpg    0\n",
       "2  b549040c49190cedc41327748aeb197c1670f14d.jpg    0\n",
       "3  4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg    0\n",
       "4  248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg    0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(test_dir+'info.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c57eb667",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [os.path.join(test_image_path, img_id) for img_id in submission.ImageID]\n",
    "test_image = pd.Series(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcc3a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, midcrop=True, transform=None):\n",
    "        self.midcrop = midcrop\n",
    "        self.data = test_image\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.midcrop:\n",
    "            image = image[64:448]\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(image)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "558ad8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "                            transforms.ToTensor()\n",
    "                        ])\n",
    "\n",
    "test_set = TestDataset(transform = test_transform)\n",
    "test_dataLodaer = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbbd1d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "model = model_vgg19.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "for images in test_dataLodaer:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
