{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1d65a3-78de-4b01-a862-e6bd4c7ef135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, Compose, CenterCrop, ColorJitter\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa8d19de-5a8e-454f-a22b-129e94792395",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 12\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f69f93c-5675-433f-8a88-7bf8e411d51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PersonID</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Class</th>\n",
       "      <th>Mask</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_Class</th>\n",
       "      <th>Has_Face</th>\n",
       "      <th>BBoxX1</th>\n",
       "      <th>...</th>\n",
       "      <th>LE_X</th>\n",
       "      <th>LE_Y</th>\n",
       "      <th>RE_X</th>\n",
       "      <th>RE_Y</th>\n",
       "      <th>N_X</th>\n",
       "      <th>N_Y</th>\n",
       "      <th>LM_X</th>\n",
       "      <th>LM_Y</th>\n",
       "      <th>RM_X</th>\n",
       "      <th>RM_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>000001</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>145</td>\n",
       "      <td>230</td>\n",
       "      <td>206</td>\n",
       "      <td>230</td>\n",
       "      <td>176</td>\n",
       "      <td>268</td>\n",
       "      <td>158</td>\n",
       "      <td>296</td>\n",
       "      <td>198</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000001</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>150</td>\n",
       "      <td>223</td>\n",
       "      <td>210</td>\n",
       "      <td>219</td>\n",
       "      <td>182</td>\n",
       "      <td>257</td>\n",
       "      <td>165</td>\n",
       "      <td>289</td>\n",
       "      <td>203</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000001</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>107</td>\n",
       "      <td>...</td>\n",
       "      <td>134</td>\n",
       "      <td>205</td>\n",
       "      <td>193</td>\n",
       "      <td>206</td>\n",
       "      <td>162</td>\n",
       "      <td>243</td>\n",
       "      <td>145</td>\n",
       "      <td>274</td>\n",
       "      <td>184</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>000001</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>110</td>\n",
       "      <td>...</td>\n",
       "      <td>139</td>\n",
       "      <td>232</td>\n",
       "      <td>209</td>\n",
       "      <td>234</td>\n",
       "      <td>171</td>\n",
       "      <td>275</td>\n",
       "      <td>151</td>\n",
       "      <td>307</td>\n",
       "      <td>194</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>000001</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>122</td>\n",
       "      <td>...</td>\n",
       "      <td>152</td>\n",
       "      <td>205</td>\n",
       "      <td>211</td>\n",
       "      <td>202</td>\n",
       "      <td>182</td>\n",
       "      <td>245</td>\n",
       "      <td>164</td>\n",
       "      <td>275</td>\n",
       "      <td>205</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>18895</td>\n",
       "      <td>006959</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>116</td>\n",
       "      <td>...</td>\n",
       "      <td>149</td>\n",
       "      <td>235</td>\n",
       "      <td>212</td>\n",
       "      <td>234</td>\n",
       "      <td>180</td>\n",
       "      <td>273</td>\n",
       "      <td>161</td>\n",
       "      <td>301</td>\n",
       "      <td>202</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18896</th>\n",
       "      <td>18896</td>\n",
       "      <td>006959</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>154</td>\n",
       "      <td>234</td>\n",
       "      <td>223</td>\n",
       "      <td>235</td>\n",
       "      <td>186</td>\n",
       "      <td>273</td>\n",
       "      <td>164</td>\n",
       "      <td>302</td>\n",
       "      <td>211</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>18897</td>\n",
       "      <td>006959</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>241</td>\n",
       "      <td>206</td>\n",
       "      <td>241</td>\n",
       "      <td>173</td>\n",
       "      <td>273</td>\n",
       "      <td>153</td>\n",
       "      <td>306</td>\n",
       "      <td>195</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>18898</td>\n",
       "      <td>006959</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>151</td>\n",
       "      <td>233</td>\n",
       "      <td>226</td>\n",
       "      <td>237</td>\n",
       "      <td>184</td>\n",
       "      <td>279</td>\n",
       "      <td>160</td>\n",
       "      <td>311</td>\n",
       "      <td>209</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>18899</td>\n",
       "      <td>006959</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>149</td>\n",
       "      <td>243</td>\n",
       "      <td>219</td>\n",
       "      <td>243</td>\n",
       "      <td>181</td>\n",
       "      <td>280</td>\n",
       "      <td>156</td>\n",
       "      <td>315</td>\n",
       "      <td>210</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18900 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 PersonID                                           Filename  \\\n",
       "0               0   000001  ../input/data/train/images/000001_female_Asian...   \n",
       "1               1   000001  ../input/data/train/images/000001_female_Asian...   \n",
       "2               2   000001  ../input/data/train/images/000001_female_Asian...   \n",
       "3               3   000001  ../input/data/train/images/000001_female_Asian...   \n",
       "4               4   000001  ../input/data/train/images/000001_female_Asian...   \n",
       "...           ...      ...                                                ...   \n",
       "18895       18895   006959  ../input/data/train/images/006959_male_Asian_1...   \n",
       "18896       18896   006959  ../input/data/train/images/006959_male_Asian_1...   \n",
       "18897       18897   006959  ../input/data/train/images/006959_male_Asian_1...   \n",
       "18898       18898   006959  ../input/data/train/images/006959_male_Asian_1...   \n",
       "18899       18899   006959  ../input/data/train/images/006959_male_Asian_1...   \n",
       "\n",
       "       Class  Mask  Gender  Age  Age_Class  Has_Face  BBoxX1  ...  LE_X  LE_Y  \\\n",
       "0          4     0       1   45          1      True     112  ...   145   230   \n",
       "1          4     0       1   45          1      True     120  ...   150   223   \n",
       "2          4     0       1   45          1      True     107  ...   134   205   \n",
       "3          4     0       1   45          1      True     110  ...   139   232   \n",
       "4         10     1       1   45          1      True     122  ...   152   205   \n",
       "...      ...   ...     ...  ...        ...       ...     ...  ...   ...   ...   \n",
       "18895      0     0       0   19          0      True     116  ...   149   235   \n",
       "18896      0     0       0   19          0      True     120  ...   154   234   \n",
       "18897      6     1       0   19          0      True     111  ...   144   241   \n",
       "18898      0     0       0   19          0      True     112  ...   151   233   \n",
       "18899     12     2       0   19          0      True     112  ...   149   243   \n",
       "\n",
       "       RE_X  RE_Y  N_X  N_Y  LM_X  LM_Y  RM_X  RM_Y  \n",
       "0       206   230  176  268   158   296   198   297  \n",
       "1       210   219  182  257   165   289   203   287  \n",
       "2       193   206  162  243   145   274   184   274  \n",
       "3       209   234  171  275   151   307   194   309  \n",
       "4       211   202  182  245   164   275   205   273  \n",
       "...     ...   ...  ...  ...   ...   ...   ...   ...  \n",
       "18895   212   234  180  273   161   301   202   300  \n",
       "18896   223   235  186  273   164   302   211   303  \n",
       "18897   206   241  173  273   153   306   195   307  \n",
       "18898   226   237  184  279   160   311   209   314  \n",
       "18899   219   243  181  280   156   315   210   315  \n",
       "\n",
       "[18900 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir_path = '/opt/ml/input/data/train/'\n",
    "train_image_path = '/opt/ml/input/data/train/images/'\n",
    "\n",
    "dt_train = pd.read_csv(train_dir_path+'expanded_train.csv')\n",
    "dt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "304c6a61-ff17-4777-8132-18def61c8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole_image_path = []\n",
    "#whole_target_label = []\n",
    "\n",
    "#for path in dt_train['path']:\n",
    "#    for file_name in [i for i in os.listdir(train_image_path+path) if './' not in i]:\n",
    "#        whole_image_path.append(train_image_path+path+'/'+file_name)\n",
    "#        whole_target_label.append((path.split('_')[1], path.split('_')[3], file_name.split('.')[0]))\n",
    "\n",
    "whole_image_path = dt_train.loc[:,\"Filename\"]\n",
    "whole_target_label = dt_train.loc[:,\"Class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8afdaf-59f1-41ce-a08c-c7bc2d2e366a",
   "metadata": {},
   "source": [
    "~~\n",
    "def onehot_enc(x):\n",
    "    def gender(i):\n",
    "        if i == 'male':\n",
    "            return 0\n",
    "        elif i == 'female':\n",
    "            return 3\n",
    "    def age(j):\n",
    "        j = int(j)\n",
    "        if j < 30:\n",
    "            return 0\n",
    "        elif j >= 30 and j < 60:\n",
    "            return 1\n",
    "        elif j >= 60:\n",
    "            return 2\n",
    "    def mask(k):\n",
    "        if k == 'normal':\n",
    "            return 12\n",
    "        elif 'incorrect' in k:\n",
    "            return 6\n",
    "        else:\n",
    "            return 0\n",
    "    return gender(x[0]) + age(x[1]) + mask(x[2])\n",
    "~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694474c7-7175-40e6-b071-e50218629f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_data = pd.Series(whole_image_path)\n",
    "sr_label = pd.Series(whole_target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2cd7b29-5f03-444e-b781-902ca74f2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize, ToTensor, Normalize, Compose, CenterCrop, ColorJitter\n",
    "from PIL import Image\n",
    "\n",
    "class Dataset_Mask(Dataset):\n",
    "    def __init__(self, encoding=True, midcrop=True, transform=None):\n",
    "        self.encoding = encoding\n",
    "        self.midcrop = midcrop\n",
    "        self.data = sr_data\n",
    "        self.label = sr_label\n",
    "        self.transform = transform\n",
    "        \n",
    "        #if encoding:\n",
    "        #    self.label = self.label.apply(onehot_enc)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(sr_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #X = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        X = np.load(self.data[idx])\n",
    "        y = self.label[idx]\n",
    "        \n",
    "        if self.midcrop:\n",
    "            # X = X[64:448]\n",
    "            x1 = dt_train.iloc[idx]['BBoxX1']\n",
    "            y1 = dt_train.iloc[idx]['BBoxY1']\n",
    "            x2 = dt_train.iloc[idx]['BBoxX2']\n",
    "            y2 = dt_train.iloc[idx]['BBoxY2']\n",
    "        X = cv2.resize(X, (224, 224))\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(X), y\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a22aff-e634-4f15-877c-752d450d01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mask = Dataset_Mask(transform = transforms.Compose([\n",
    "                                    ToTensor(),\n",
    "                                    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5788c44b-07ac-4820-a22d-b6ffa3fc81f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset_mask) * 0.8)\n",
    "val_size = len(dataset_mask) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e285cb6-ee37-4a0c-a804-7506dcfebc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size : 15120\n",
      "validation data size : 3780\n"
     ]
    }
   ],
   "source": [
    "mask_train_set, mask_val_set = torch.utils.data.random_split(dataset_mask, [train_size, val_size])\n",
    "print(f'training data size : {len(mask_train_set)}')\n",
    "print(f'validation data size : {len(mask_val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37b1ced0-1e34-4d50-b2b2-dcdff8fd33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_dataloader_mask = DataLoader(dataset = mask_train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_dataloader_mask = DataLoader(dataset = mask_val_set, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af71a4ef-a137-47b0-af94-eebedaa36014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /opt/ml/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c856c32befa4065a4754b366880a76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=87306240.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "필요 입력 채널 개수 3\n",
      "네트워크 출력 채널 개수 1000\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "basemodel_resnet34 = torchvision.models.resnet34(pretrained=True)\n",
    "print('필요 입력 채널 개수', basemodel_resnet34.conv1.weight.shape[1])\n",
    "print('네트워크 출력 채널 개수', basemodel_resnet34.fc.weight.shape[0])\n",
    "print(basemodel_resnet34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11357fc4-548b-452e-bbd8-e60ef6261308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필요 입력 채널 개수 3\n",
      "네트워크 출력 채널 개수 18\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "class_num = 18\n",
    "basemodel_resnet34.fc = nn.Linear(in_features=512, out_features=class_num, bias=True)\n",
    "nn.init.xavier_uniform_(basemodel_resnet34.fc.weight)\n",
    "stdv = 1. / math.sqrt(basemodel_resnet34.fc.weight.size(1))\n",
    "basemodel_resnet34.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "print('필요 입력 채널 개수', basemodel_resnet34.conv1.weight.shape[1])\n",
    "print('네트워크 출력 채널 개수', basemodel_resnet34.fc.weight.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a1a2262-f820-4294-8086-3a48a5e5d686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")\n",
    "\n",
    "basemodel_resnet34.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCH = 30\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(basemodel_resnet34.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64a36f92-aad5-4bbb-a920-29d50f0f058b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] name:[conv1.weight] shape:[(64, 3, 7, 7)].\n",
      "    val:[ 0.005 -0.007  0.008  0.038  0.049]\n",
      "[1] name:[bn1.weight] shape:[(64,)].\n",
      "    val:[0.302 0.268 0.26  0.311 0.238]\n",
      "[2] name:[bn1.bias] shape:[(64,)].\n",
      "    val:[0.481 0.207 0.331 0.38  0.094]\n",
      "[3] name:[layer1.0.conv1.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[-0.005  0.015 -0.006 -0.06  -0.024]\n",
      "[4] name:[layer1.0.bn1.weight] shape:[(64,)].\n",
      "    val:[0.24  0.185 0.216 0.165 0.181]\n",
      "[5] name:[layer1.0.bn1.bias] shape:[(64,)].\n",
      "    val:[0.025 0.088 0.082 0.142 0.066]\n",
      "[6] name:[layer1.0.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 0.066 -0.01   0.041  0.033 -0.055]\n",
      "[7] name:[layer1.0.bn2.weight] shape:[(64,)].\n",
      "    val:[0.34  0.187 0.252 0.307 0.259]\n",
      "[8] name:[layer1.0.bn2.bias] shape:[(64,)].\n",
      "    val:[-0.251  0.196  0.23  -0.114  0.07 ]\n",
      "[9] name:[layer1.1.conv1.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[-0.008 -0.04  -0.054 -0.019  0.011]\n",
      "[10] name:[layer1.1.bn1.weight] shape:[(64,)].\n",
      "    val:[0.178 0.373 0.18  0.26  0.246]\n",
      "[11] name:[layer1.1.bn1.bias] shape:[(64,)].\n",
      "    val:[ 0.073 -0.222  0.177 -0.063 -0.051]\n",
      "[12] name:[layer1.1.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[-0.002 -0.04  -0.012  0.008  0.088]\n",
      "[13] name:[layer1.1.bn2.weight] shape:[(64,)].\n",
      "    val:[0.417 0.209 0.224 0.179 0.402]\n",
      "[14] name:[layer1.1.bn2.bias] shape:[(64,)].\n",
      "    val:[-0.033 -0.08   0.174 -0.102  0.176]\n",
      "[15] name:[layer1.2.conv1.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 0.021 -0.101 -0.024  0.013  0.111]\n",
      "[16] name:[layer1.2.bn1.weight] shape:[(64,)].\n",
      "    val:[0.335 0.203 0.192 0.247 0.248]\n",
      "[17] name:[layer1.2.bn1.bias] shape:[(64,)].\n",
      "    val:[-0.266 -0.041 -0.11  -0.246  0.017]\n",
      "[18] name:[layer1.2.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[-0.017  0.023 -0.021 -0.04   0.042]\n",
      "[19] name:[layer1.2.bn2.weight] shape:[(64,)].\n",
      "    val:[0.554 0.169 0.286 0.193 0.248]\n",
      "[20] name:[layer1.2.bn2.bias] shape:[(64,)].\n",
      "    val:[-0.093 -0.217  0.121  0.058  0.072]\n",
      "[21] name:[layer2.0.conv1.weight] shape:[(128, 64, 3, 3)].\n",
      "    val:[-0.007  0.001 -0.007 -0.024  0.034]\n",
      "[22] name:[layer2.0.bn1.weight] shape:[(128,)].\n",
      "    val:[0.261 0.288 0.174 0.238 0.274]\n",
      "[23] name:[layer2.0.bn1.bias] shape:[(128,)].\n",
      "    val:[-0.083 -0.094  0.286 -0.043 -0.101]\n",
      "[24] name:[layer2.0.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.005 -0.025  0.025  0.007 -0.021]\n",
      "[25] name:[layer2.0.bn2.weight] shape:[(128,)].\n",
      "    val:[0.376 0.01  0.186 0.269 0.344]\n",
      "[26] name:[layer2.0.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.068  0.218  0.066  0.078  0.135]\n",
      "[27] name:[layer2.0.downsample.0.weight] shape:[(128, 64, 1, 1)].\n",
      "    val:[-0.008 -0.109  0.045 -0.033 -0.003]\n",
      "[28] name:[layer2.0.downsample.1.weight] shape:[(128,)].\n",
      "    val:[0.169 0.36  0.406 0.076 0.207]\n",
      "[29] name:[layer2.0.downsample.1.bias] shape:[(128,)].\n",
      "    val:[-0.068  0.218  0.066  0.078  0.135]\n",
      "[30] name:[layer2.1.conv1.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.011 -0.004  0.012 -0.015 -0.011]\n",
      "[31] name:[layer2.1.bn1.weight] shape:[(128,)].\n",
      "    val:[0.157 0.302 0.154 0.311 0.211]\n",
      "[32] name:[layer2.1.bn1.bias] shape:[(128,)].\n",
      "    val:[ 0.063 -0.199  0.054 -0.259 -0.036]\n",
      "[33] name:[layer2.1.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[ 0.022 -0.006  0.029  0.001 -0.023]\n",
      "[34] name:[layer2.1.bn2.weight] shape:[(128,)].\n",
      "    val:[0.146 0.271 0.169 0.162 0.091]\n",
      "[35] name:[layer2.1.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.064 -0.481 -0.127 -0.03  -0.03 ]\n",
      "[36] name:[layer2.2.conv1.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[ 0.039  0.004 -0.007  0.008  0.01 ]\n",
      "[37] name:[layer2.2.bn1.weight] shape:[(128,)].\n",
      "    val:[0.28  0.303 0.248 0.216 0.235]\n",
      "[38] name:[layer2.2.bn1.bias] shape:[(128,)].\n",
      "    val:[-0.211 -0.532 -0.127 -0.146 -0.088]\n",
      "[39] name:[layer2.2.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.04  -0.046 -0.016 -0.017  0.006]\n",
      "[40] name:[layer2.2.bn2.weight] shape:[(128,)].\n",
      "    val:[0.25  0.102 0.138 0.317 0.144]\n",
      "[41] name:[layer2.2.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.051  0.006  0.007 -0.147  0.035]\n",
      "[42] name:[layer2.3.conv1.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.016 -0.033  0.011 -0.001 -0.038]\n",
      "[43] name:[layer2.3.bn1.weight] shape:[(128,)].\n",
      "    val:[0.216 0.181 0.23  0.205 0.245]\n",
      "[44] name:[layer2.3.bn1.bias] shape:[(128,)].\n",
      "    val:[-0.179 -0.142 -0.17  -0.189 -0.196]\n",
      "[45] name:[layer2.3.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[ 0.012 -0.026 -0.027 -0.002 -0.029]\n",
      "[46] name:[layer2.3.bn2.weight] shape:[(128,)].\n",
      "    val:[ 0.194 -0.018  0.164  0.281  0.151]\n",
      "[47] name:[layer2.3.bn2.bias] shape:[(128,)].\n",
      "    val:[ 0.117  0.032 -0.26  -0.127 -0.086]\n",
      "[48] name:[layer3.0.conv1.weight] shape:[(256, 128, 3, 3)].\n",
      "    val:[ 0.007 -0.013  0.008 -0.018 -0.007]\n",
      "[49] name:[layer3.0.bn1.weight] shape:[(256,)].\n",
      "    val:[0.274 0.267 0.268 0.232 0.234]\n",
      "[50] name:[layer3.0.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.06  -0.081 -0.092  0.066  0.037]\n",
      "[51] name:[layer3.0.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.007  0.005 -0.012 -0.008 -0.003]\n",
      "[52] name:[layer3.0.bn2.weight] shape:[(256,)].\n",
      "    val:[0.349 0.329 0.257 0.217 0.355]\n",
      "[53] name:[layer3.0.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.097 -0.09   0.072  0.154 -0.044]\n",
      "[54] name:[layer3.0.downsample.0.weight] shape:[(256, 128, 1, 1)].\n",
      "    val:[-0.022  0.023  0.002 -0.014 -0.019]\n",
      "[55] name:[layer3.0.downsample.1.weight] shape:[(256,)].\n",
      "    val:[0.143 0.159 0.071 0.082 0.114]\n",
      "[56] name:[layer3.0.downsample.1.bias] shape:[(256,)].\n",
      "    val:[-0.097 -0.09   0.072  0.154 -0.044]\n",
      "[57] name:[layer3.1.conv1.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.001 -0.01  -0.013  0.001 -0.002]\n",
      "[58] name:[layer3.1.bn1.weight] shape:[(256,)].\n",
      "    val:[0.22  0.208 0.202 0.218 0.215]\n",
      "[59] name:[layer3.1.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.185 -0.167 -0.168 -0.18  -0.068]\n",
      "[60] name:[layer3.1.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.004  0.004  0.016 -0.006 -0.01 ]\n",
      "[61] name:[layer3.1.bn2.weight] shape:[(256,)].\n",
      "    val:[0.25  0.2   0.122 0.093 0.191]\n",
      "[62] name:[layer3.1.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.137 -0.085 -0.053 -0.143 -0.108]\n",
      "[63] name:[layer3.2.conv1.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.039 -0.005  0.003  0.02   0.02 ]\n",
      "[64] name:[layer3.2.bn1.weight] shape:[(256,)].\n",
      "    val:[0.234 0.232 0.264 0.187 0.217]\n",
      "[65] name:[layer3.2.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.25  -0.252 -0.323 -0.165 -0.182]\n",
      "[66] name:[layer3.2.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.009 -0.025  0.01   0.021 -0.024]\n",
      "[67] name:[layer3.2.bn2.weight] shape:[(256,)].\n",
      "    val:[0.304 0.184 0.146 0.06  0.261]\n",
      "[68] name:[layer3.2.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.163 -0.158 -0.014 -0.    -0.137]\n",
      "[69] name:[layer3.3.conv1.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.038 -0.003  0.034 -0.018 -0.012]\n",
      "[70] name:[layer3.3.bn1.weight] shape:[(256,)].\n",
      "    val:[0.212 0.206 0.159 0.265 0.208]\n",
      "[71] name:[layer3.3.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.225 -0.162 -0.17  -0.415 -0.243]\n",
      "[72] name:[layer3.3.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.01  -0.014  0.006 -0.005  0.017]\n",
      "[73] name:[layer3.3.bn2.weight] shape:[(256,)].\n",
      "    val:[0.399 0.206 0.196 0.065 0.27 ]\n",
      "[74] name:[layer3.3.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.316 -0.16  -0.087  0.015 -0.196]\n",
      "[75] name:[layer3.4.conv1.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.002 -0.023 -0.029  0.015 -0.   ]\n",
      "[76] name:[layer3.4.bn1.weight] shape:[(256,)].\n",
      "    val:[0.196 0.196 0.241 0.226 0.21 ]\n",
      "[77] name:[layer3.4.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.207 -0.197 -0.268 -0.29  -0.261]\n",
      "[78] name:[layer3.4.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.005  0.001 -0.005  0.028  0.014]\n",
      "[79] name:[layer3.4.bn2.weight] shape:[(256,)].\n",
      "    val:[0.322 0.201 0.109 0.01  0.282]\n",
      "[80] name:[layer3.4.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.196 -0.123 -0.071 -0.001 -0.24 ]\n",
      "[81] name:[layer3.5.conv1.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.016  0.005 -0.004  0.028  0.03 ]\n",
      "[82] name:[layer3.5.bn1.weight] shape:[(256,)].\n",
      "    val:[0.287 0.194 0.268 0.239 0.229]\n",
      "[83] name:[layer3.5.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.274 -0.299 -0.355 -0.27  -0.238]\n",
      "[84] name:[layer3.5.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.002  0.012  0.006 -0.007 -0.005]\n",
      "[85] name:[layer3.5.bn2.weight] shape:[(256,)].\n",
      "    val:[ 0.358  0.252  0.161 -0.03   0.292]\n",
      "[86] name:[layer3.5.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.317 -0.187  0.042 -0.006 -0.33 ]\n",
      "[87] name:[layer4.0.conv1.weight] shape:[(512, 256, 3, 3)].\n",
      "    val:[0.023 0.048 0.058 0.017 0.017]\n",
      "[88] name:[layer4.0.bn1.weight] shape:[(512,)].\n",
      "    val:[0.275 0.262 0.282 0.279 0.251]\n",
      "[89] name:[layer4.0.bn1.bias] shape:[(512,)].\n",
      "    val:[-0.271 -0.201 -0.171 -0.224 -0.2  ]\n",
      "[90] name:[layer4.0.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[ 0.001  0.001 -0.002 -0.013 -0.018]\n",
      "[91] name:[layer4.0.bn2.weight] shape:[(512,)].\n",
      "    val:[0.7   0.664 0.803 0.695 0.743]\n",
      "[92] name:[layer4.0.bn2.bias] shape:[(512,)].\n",
      "    val:[-0.072 -0.131 -0.151 -0.076 -0.066]\n",
      "[93] name:[layer4.0.downsample.0.weight] shape:[(512, 256, 1, 1)].\n",
      "    val:[-0.004 -0.033  0.005  0.037 -0.014]\n",
      "[94] name:[layer4.0.downsample.1.weight] shape:[(512,)].\n",
      "    val:[0.325 0.38  0.501 0.42  0.451]\n",
      "[95] name:[layer4.0.downsample.1.bias] shape:[(512,)].\n",
      "    val:[-0.072 -0.131 -0.151 -0.076 -0.066]\n",
      "[96] name:[layer4.1.conv1.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[ 0.005 -0.005  0.007  0.003  0.003]\n",
      "[97] name:[layer4.1.bn1.weight] shape:[(512,)].\n",
      "    val:[0.247 0.23  0.21  0.258 0.288]\n",
      "[98] name:[layer4.1.bn1.bias] shape:[(512,)].\n",
      "    val:[-0.269 -0.243 -0.198 -0.25  -0.327]\n",
      "[99] name:[layer4.1.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[0.002 0.009 0.004 0.002 0.008]\n",
      "[100] name:[layer4.1.bn2.weight] shape:[(512,)].\n",
      "    val:[0.592 0.595 0.524 0.572 0.548]\n",
      "[101] name:[layer4.1.bn2.bias] shape:[(512,)].\n",
      "    val:[-0.142 -0.179 -0.186 -0.113 -0.115]\n",
      "[102] name:[layer4.2.conv1.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[0.021 0.028 0.021 0.049 0.059]\n",
      "[103] name:[layer4.2.bn1.weight] shape:[(512,)].\n",
      "    val:[0.26  0.228 0.275 0.229 0.224]\n",
      "[104] name:[layer4.2.bn1.bias] shape:[(512,)].\n",
      "    val:[-0.215 -0.164 -0.275 -0.147 -0.161]\n",
      "[105] name:[layer4.2.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[0.042 0.035 0.04  0.032 0.017]\n",
      "[106] name:[layer4.2.bn2.weight] shape:[(512,)].\n",
      "    val:[1.436 1.512 1.565 1.291 1.287]\n",
      "[107] name:[layer4.2.bn2.bias] shape:[(512,)].\n",
      "    val:[0.122 0.129 0.193 0.133 0.098]\n",
      "[108] name:[fc.weight] shape:[(18, 512)].\n",
      "    val:[ 0.072 -0.057  0.056 -0.099 -0.003]\n",
      "[109] name:[fc.bias] shape:[(18,)].\n",
      "    val:[-0.035  0.008  0.029 -0.013  0.   ]\n",
      "Total number of parameters:[21,293,906].\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "n_param = 0\n",
    "for p_idx, (param_name, param) in enumerate(basemodel_resnet34.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        param_numpy = param.detach().cpu().numpy()\n",
    "        n_param += len(param_numpy.reshape(-1))\n",
    "        print (\"[%d] name:[%s] shape:[%s].\"%(p_idx,param_name,param_numpy.shape))\n",
    "        print (\"    val:%s\"%(param_numpy.reshape(-1)[:5]))\n",
    "print (\"Total number of parameters:[%s].\"%(format(n_param,',d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c067d0e-38c9-4ab8-baba-88cf8dcb8c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[0/30] training loss 0.016, training accuracy 0.035\n",
      "epoch[0/30] training loss 0.012, training accuracy 0.137\n",
      "epoch[0/30] training loss 0.009, training accuracy 0.336\n",
      "epoch[0/30] training loss 0.008, training accuracy 0.441\n",
      "epoch[0/30] training loss 0.007, training accuracy 0.574\n",
      "epoch[0/30] training loss 0.006, training accuracy 0.629\n",
      "epoch[0/30] training loss 0.005, training accuracy 0.660\n",
      "epoch[0/30] training loss 0.004, training accuracy 0.746\n",
      "epoch[0/30] training loss 0.003, training accuracy 0.730\n",
      "epoch[0/30] training loss 0.004, training accuracy 0.711\n",
      "epoch[0/30] training loss 0.003, training accuracy 0.805\n",
      "epoch[0/30] training loss 0.003, training accuracy 0.777\n",
      "epoch[0/30] training loss 0.003, training accuracy 0.727\n",
      "epoch[0/30] training loss 0.002, training accuracy 0.824\n",
      "epoch[0/30] training loss 0.003, training accuracy 0.797\n",
      "epoch[0/30] training loss 0.002, training accuracy 0.805\n",
      "epoch[0/30] training loss 0.002, training accuracy 0.852\n",
      "epoch[0/30] training loss 0.002, training accuracy 0.852\n",
      "epoch[0/30] training loss 0.002, training accuracy 0.820\n",
      "epoch[0/30] training loss 0.002, training accuracy 0.887\n",
      "epoch[0/30] training loss 0.002, training accuracy 0.871\n",
      "epoch[0/30] training loss 0.002, training accuracy 0.871\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.891\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.906\n",
      "epoch[0/30] training loss 0.002, training accuracy 0.867\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.914\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.887\n",
      "epoch[0/30] training loss 0.002, training accuracy 0.855\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.875\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.887\n",
      "epoch[0/30] training loss 0.002, training accuracy 0.871\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.898\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.918\n",
      "epoch[0/30] training loss 0.002, training accuracy 0.867\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.930\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.895\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.910\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.926\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.914\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.906\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.906\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.926\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.891\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.914\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.910\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.910\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.926\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.938\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.906\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.938\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.918\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.918\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.922\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.926\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.941\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.930\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.910\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.945\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.926\n",
      "epoch[0/30] training loss 0.001, training accuracy 0.059\n",
      "[val] acc : 0.910, loss : 0.260\n",
      "best acc : 0.910, best loss : 0.260\n",
      "epoch[1/30] training loss 0.001, training accuracy 0.957\n",
      "epoch[1/30] training loss 0.001, training accuracy 0.930\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.969\n",
      "epoch[1/30] training loss 0.001, training accuracy 0.941\n",
      "epoch[1/30] training loss 0.001, training accuracy 0.922\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.973\n",
      "epoch[1/30] training loss 0.001, training accuracy 0.945\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.961\n",
      "epoch[1/30] training loss 0.001, training accuracy 0.949\n",
      "epoch[1/30] training loss 0.001, training accuracy 0.941\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.949\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.965\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.965\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.965\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.988\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.965\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.961\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.977\n",
      "epoch[1/30] training loss 0.001, training accuracy 0.949\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.973\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.961\n",
      "epoch[1/30] training loss 0.001, training accuracy 0.957\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.977\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.969\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.980\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.965\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.977\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[1/30] training loss 0.001, training accuracy 0.945\n",
      "epoch[1/30] training loss 0.001, training accuracy 0.961\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.969\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.977\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.980\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.973\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.977\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.969\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.977\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.965\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.973\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.969\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.977\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.988\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.965\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.977\n",
      "epoch[1/30] training loss 0.001, training accuracy 0.953\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.973\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.980\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.977\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.973\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.980\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.977\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.969\n",
      "epoch[1/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[1/30] training loss 0.001, training accuracy 0.059\n",
      "[val] acc : 0.948, loss : 0.160\n",
      "best acc : 0.948, best loss : 0.160\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.988\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.001, training accuracy 0.961\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.988\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.977\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.988\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.988\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.988\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.988\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[2/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[2/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[2/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[2/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[2/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.988\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.988\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.988\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.988\n",
      "epoch[2/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[2/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[2/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[2/30] training loss 0.000, training accuracy 0.059\n",
      "[val] acc : 0.965, loss : 0.115\n",
      "best acc : 0.965, best loss : 0.115\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.988\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.977\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.988\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.977\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[3/30] training loss 0.001, training accuracy 0.059\n",
      "[val] acc : 0.966, loss : 0.104\n",
      "best acc : 0.966, best loss : 0.104\n",
      "epoch[4/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[4/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[4/30] training loss 0.001, training accuracy 0.969\n",
      "epoch[4/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[4/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[4/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[4/30] training loss 0.000, training accuracy 0.988\n",
      "epoch[4/30] training loss 0.000, training accuracy 0.984\n",
      "epoch[4/30] training loss 0.000, training accuracy 0.980\n",
      "epoch[4/30] training loss 0.000, training accuracy 0.977\n",
      "epoch[4/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[4/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[4/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[4/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[4/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[4/30] training loss 0.000, training accuracy 0.988\n",
      "epoch[4/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[4/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[4/30] training loss 0.000, training accuracy 1.000\n",
      "epoch[4/30] training loss 0.000, training accuracy 0.992\n",
      "epoch[4/30] training loss 0.000, training accuracy 0.996\n",
      "epoch[4/30] training loss 0.000, training accuracy 0.996\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25574/1165031955.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmatches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    basemodel_resnet34.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    for train_batch in train_dataloader_mask:\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outs = basemodel_resnet34(inputs)\n",
    "        preds = torch.argmax(outs, dim=-1)\n",
    "        loss = criterion(outs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        \n",
    "        train_loss = loss_value / batch_size\n",
    "        train_acc = matches / batch_size\n",
    "        print(f\"epoch[{epoch}/{NUM_EPOCH}] training loss {train_loss:.3f}, training accuracy {train_acc:.3f}\")\n",
    "        \n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        basemodel_resnet34.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        for val_batch in val_dataloader_mask:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outs = basemodel_resnet34(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            \n",
    "            loss_item = criterion(outs, labels).item()\n",
    "            acc_item = (labels==preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "            \n",
    "        val_loss = np.sum(val_loss_items) / len(val_dataloader_mask)\n",
    "        val_acc = np.sum(val_acc_items) / len(mask_val_set)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "        print(f\"[val] acc : {val_acc:.3f}, loss : {val_loss:.3f}\")\n",
    "        print(f\"best acc : {best_val_acc:.3f}, best loss : {best_val_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afda23d0-6ad9-408b-99c9-705ae5fd8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "test_dir_path = '/opt/ml/input/data/eval/'\n",
    "test_image_path = '/opt/ml/input/data/eval/images/'\n",
    "\n",
    "submission = pd.read_csv(test_dir_path+'info.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c2df1-a083-4e27-a655-f2c73d15f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [os.path.join(test_image_path, img_id) for img_id in submission.ImageID]\n",
    "test_image = pd.Series(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c14322-45a8-40a5-aded-f335204484b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Dataset(Dataset):\n",
    "    def __init__(self, midcrop=True, transform=None):\n",
    "        self.midcrop = midcrop\n",
    "        self.data = test_image\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(test_image)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.midcrop:\n",
    "            img = img[64:448]\n",
    "            \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138db932-cc09-4f85-89d8-e7485659f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Test_Dataset(transform = transforms.Compose([\n",
    "                            transforms.ToTensor()\n",
    "                        ]))\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = basemodel_resnet34.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir_path, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd05b0a-17a2-47b4-b10d-de6e4344e098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
